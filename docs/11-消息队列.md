# 消息队列深度解析

## 目录
- [一、消息队列原理深度解析](#一消息队列原理深度解析)
- [二、Kafka深度原理](#二kafka深度原理)
- [三、RocketMQ深度原理](#三rocketmq深度原理)
- [四、消息可靠性机制](#四消息可靠性机制)
- [五、性能优化深度解析](#五性能优化深度解析)
- [六、消息队列对比](#六消息队列对比)
- [七、高频面试题](#七高频面试题)

## 一、消息队列原理深度解析

### 1.1 消息队列架构原理

#### 1.1.1 消息队列核心组件

**消息队列架构图**：
```
┌─────────────────────────────────────────────────────────┐
│                    消息队列系统                          │
├─────────────────────────────────────────────────────────┤
│  Producer Layer (生产者层)    │  Message Broker (消息代理) │
│  ┌─────────────────────┐  │  ┌─────────────────────┐    │
│  │   Message Sender    │  │  │   Message Router    │    │
│  │   Serializer        │  │  │   Message Store     │    │
│  │   Load Balancer     │  │  │   Message Queue     │    │
│  └─────────────────────┘  │  └─────────────────────┘    │
├─────────────────────────────────────────────────────────┤
│  Consumer Layer (消费者层)    │  Storage Layer (存储层)    │
│  ┌─────────────────────┐  │  ┌─────────────────────┐    │
│  │   Message Receiver  │  │  │   Disk Storage      │    │
│  │   Deserializer      │  │  │   Memory Cache      │    │
│  │   Message Processor │  │  │   Index Files       │    │
│  └─────────────────────┘  │  └─────────────────────┘    │
└─────────────────────────────────────────────────────────┘
```

#### 1.1.2 消息流转原理

**消息流转过程**：
```java
// 消息流转核心实现
public class MessageFlowProcessor {
    
    // 1. 消息发送流程
    public void sendMessage(Message message) {
        // 序列化消息
        byte[] serializedMessage = serialize(message);
        
        // 路由到目标队列
        String targetQueue = routeMessage(message);
        
        // 持久化消息
        persistMessage(targetQueue, serializedMessage);
        
        // 通知消费者
        notifyConsumers(targetQueue);
    }
    
    // 2. 消息消费流程
    public void consumeMessage(String queueName) {
        // 从队列获取消息
        Message message = pollMessage(queueName);
        
        if (message != null) {
            // 反序列化
            Object payload = deserialize(message.getBody());
            
            // 处理消息
            processMessage(payload);
            
            // 确认消费
            acknowledgeMessage(message.getId());
        }
    }
    
    // 3. 消息路由算法
    private String routeMessage(Message message) {
        // 基于主题路由
        if (message.getTopic() != null) {
            return getTopicQueue(message.getTopic());
        }
        
        // 基于分区路由
        if (message.getPartition() != null) {
            return getPartitionQueue(message.getPartition());
        }
        
        // 基于负载均衡路由
        return getLoadBalancedQueue();
    }
}
```

#### 1.1.3 消息存储原理

**消息存储架构**：
```java
// 消息存储核心实现
public class MessageStorage {
    
    // 存储引擎接口
    public interface StorageEngine {
        void write(Message message);
        Message read(String messageId);
        void delete(String messageId);
        List<Message> scan(String queueName, long offset, int limit);
    }
    
    // 基于文件的存储引擎
    public class FileStorageEngine implements StorageEngine {
        private final String dataDir;
        private final Map<String, FileChannel> channels;
        
        @Override
        public void write(Message message) {
            String queueName = message.getQueueName();
            FileChannel channel = getChannel(queueName);
            
            // 写入消息头
            ByteBuffer header = ByteBuffer.allocate(8);
            header.putLong(message.getTimestamp());
            channel.write(header);
            
            // 写入消息体
            ByteBuffer body = ByteBuffer.wrap(message.getBody());
            channel.write(body);
            
            // 强制刷盘
            channel.force(false);
        }
        
        @Override
        public Message read(String messageId) {
            // 从索引文件查找消息位置
            long offset = getMessageOffset(messageId);
            
            // 从数据文件读取消息
            return readMessageAtOffset(offset);
        }
    }
    
    // 基于内存的存储引擎
    public class MemoryStorageEngine implements StorageEngine {
        private final Map<String, Queue<Message>> queues;
        private final Map<String, Message> messageIndex;
        
        @Override
        public void write(Message message) {
            String queueName = message.getQueueName();
            Queue<Message> queue = queues.computeIfAbsent(queueName, k -> new ConcurrentLinkedQueue<>());
            queue.offer(message);
            messageIndex.put(message.getId(), message);
        }
        
        @Override
        public Message read(String messageId) {
            return messageIndex.get(messageId);
        }
    }
}
```

### 1.2 消息队列核心算法

#### 1.2.1 消息分区算法

**分区算法实现**：
```java
// 消息分区算法
public class MessagePartitioner {
    
    // 1. 哈希分区算法
    public int hashPartition(String key, int partitionCount) {
        return Math.abs(key.hashCode()) % partitionCount;
    }
    
    // 2. 轮询分区算法
    private int roundRobinCounter = 0;
    public int roundRobinPartition(int partitionCount) {
        return (roundRobinCounter++) % partitionCount;
    }
    
    // 3. 一致性哈希分区算法
    public class ConsistentHashPartitioner {
        private final TreeMap<Long, Integer> hashRing;
        private final int virtualNodes;
        
        public ConsistentHashPartitioner(int partitionCount, int virtualNodes) {
            this.virtualNodes = virtualNodes;
            this.hashRing = new TreeMap<>();
            
            // 初始化哈希环
            for (int i = 0; i < partitionCount; i++) {
                for (int j = 0; j < virtualNodes; j++) {
                    String virtualNode = i + "-" + j;
                    long hash = hash(virtualNode);
                    hashRing.put(hash, i);
                }
            }
        }
        
        public int getPartition(String key) {
            long hash = hash(key);
            Map.Entry<Long, Integer> entry = hashRing.ceilingEntry(hash);
            if (entry == null) {
                entry = hashRing.firstEntry();
            }
            return entry.getValue();
        }
        
        private long hash(String key) {
            return key.hashCode() & 0x7FFFFFFF;
        }
    }
}
```

#### 1.2.2 消息顺序保证算法

**顺序保证实现**：
```java
// 消息顺序保证算法
public class MessageOrdering {
    
    // 1. 全局顺序保证
    public class GlobalOrdering {
        private final AtomicLong sequenceNumber = new AtomicLong(0);
        private final Map<String, Long> messageSequences = new ConcurrentHashMap<>();
        
        public long assignSequence(String messageId) {
            long sequence = sequenceNumber.incrementAndGet();
            messageSequences.put(messageId, sequence);
            return sequence;
        }
        
        public boolean isInOrder(String messageId) {
            Long sequence = messageSequences.get(messageId);
            return sequence != null && sequence == getExpectedSequence();
        }
    }
    
    // 2. 分区顺序保证
    public class PartitionOrdering {
        private final Map<String, AtomicLong> partitionSequences = new ConcurrentHashMap<>();
        
        public long assignPartitionSequence(String partitionKey) {
            AtomicLong sequence = partitionSequences.computeIfAbsent(partitionKey, k -> new AtomicLong(0));
            return sequence.incrementAndGet();
        }
        
        public boolean isPartitionInOrder(String partitionKey, long sequence) {
            AtomicLong expectedSequence = partitionSequences.get(partitionKey);
            return expectedSequence != null && sequence == expectedSequence.get();
        }
    }
    
    // 3. 时间戳顺序保证
    public class TimestampOrdering {
        private final Map<String, Long> lastTimestamps = new ConcurrentHashMap<>();
        
        public boolean isTimestampOrdered(String partitionKey, long timestamp) {
            Long lastTimestamp = lastTimestamps.get(partitionKey);
            if (lastTimestamp == null || timestamp >= lastTimestamp) {
                lastTimestamps.put(partitionKey, timestamp);
                return true;
            }
            return false;
        }
    }
}
```

### 1.3 消息队列性能优化原理

#### 1.3.1 零拷贝技术

**零拷贝实现原理**：
```java
// 零拷贝技术实现
public class ZeroCopyOptimization {
    
    // 1. 使用FileChannel.transferTo()
    public void transferTo(FileChannel source, FileChannel target) throws IOException {
        long position = 0;
        long count = source.size();
        
        while (position < count) {
            long transferred = source.transferTo(position, count - position, target);
            position += transferred;
        }
    }
    
    // 2. 使用MappedByteBuffer
    public void memoryMap(FileChannel channel) throws IOException {
        MappedByteBuffer buffer = channel.map(
            FileChannel.MapMode.READ_WRITE, 
            0, 
            channel.size()
        );
        
        // 直接操作内存映射
        buffer.put(new byte[]{1, 2, 3, 4});
        buffer.force(); // 强制刷盘
    }
    
    // 3. 使用DirectByteBuffer
    public void directBuffer() {
        ByteBuffer directBuffer = ByteBuffer.allocateDirect(1024);
        // 直接内存操作，避免JVM堆内存拷贝
    }
}
```

#### 1.3.2 批量处理优化

**批量处理实现**：
```java
// 批量处理优化
public class BatchProcessing {
    
    // 1. 批量发送
    public class BatchSender {
        private final List<Message> batch = new ArrayList<>();
        private final int batchSize;
        private final long batchTimeout;
        
        public void send(Message message) {
            batch.add(message);
            
            if (batch.size() >= batchSize) {
                flushBatch();
            }
        }
        
        public void flushBatch() {
            if (!batch.isEmpty()) {
                sendBatch(batch);
                batch.clear();
            }
        }
    }
    
    // 2. 批量消费
    public class BatchConsumer {
        private final int batchSize;
        private final long batchTimeout;
        
        public List<Message> consumeBatch(String queueName) {
            List<Message> messages = new ArrayList<>();
            long startTime = System.currentTimeMillis();
            
            while (messages.size() < batchSize && 
                   (System.currentTimeMillis() - startTime) < batchTimeout) {
                Message message = pollMessage(queueName);
                if (message != null) {
                    messages.add(message);
                } else {
                    Thread.yield(); // 让出CPU
                }
            }
            
            return messages;
        }
    }
    
    // 3. 批量确认
    public class BatchAcknowledger {
        private final Set<String> pendingAcks = new ConcurrentHashMap<String, Boolean>().keySet();
        private final int batchSize;
        
        public void acknowledge(String messageId) {
            pendingAcks.add(messageId);
            
            if (pendingAcks.size() >= batchSize) {
                flushAcks();
            }
        }
        
        public void flushAcks() {
            if (!pendingAcks.isEmpty()) {
                sendAcks(new ArrayList<>(pendingAcks));
                pendingAcks.clear();
            }
        }
    }
}
```

## 二、Kafka深度原理

### 2.1 Kafka架构深度解析

#### 2.1.1 Kafka核心组件架构

**Kafka架构图**：
```
┌─────────────────────────────────────────────────────────┐
│                    Kafka集群架构                        │
├─────────────────────────────────────────────────────────┤
│  Producer Layer (生产者层)    │  Broker Layer (代理层)     │
│  ┌─────────────────────┐  │  ┌─────────────────────┐    │
│  │   Producer Client   │  │  │   Broker Node 1    │    │
│  │   Serializer        │  │  │   Broker Node 2    │    │
│  │   Partitioner       │  │  │   Broker Node 3    │    │
│  └─────────────────────┘  │  └─────────────────────┘    │
├─────────────────────────────────────────────────────────┤
│  Consumer Layer (消费者层)    │  Storage Layer (存储层)    │
│  ┌─────────────────────┐  │  ┌─────────────────────┐    │
│  │   Consumer Group    │  │  │   Partition Files   │    │
│  │   Consumer Client   │  │  │   Index Files       │    │
│  │   Offset Manager    │  │  │   Log Segments      │    │
│  └─────────────────────┘  │  └─────────────────────┘    │
└─────────────────────────────────────────────────────────┘
```

#### 2.1.2 Kafka存储原理深度解析

**Kafka存储架构**：
```java
// Kafka存储核心实现
public class KafkaStorage {
    
    // 1. 日志段管理
    public class LogSegment {
        private final FileChannel channel;
        private final long baseOffset;
        private final long size;
        private final long maxTimestamp;
        
        // 写入消息到日志段
        public void append(RecordBatch batch) throws IOException {
            ByteBuffer buffer = batch.serialize();
            int written = channel.write(buffer);
            
            // 更新索引
            updateIndex(batch.getBaseOffset(), written);
        }
        
        // 读取消息
        public RecordBatch read(long offset, int maxSize) throws IOException {
            long position = getPosition(offset);
            ByteBuffer buffer = ByteBuffer.allocate(maxSize);
            channel.read(buffer, position);
            return RecordBatch.deserialize(buffer);
        }
    }
    
    // 2. 分区管理
    public class Partition {
        private final String topic;
        private final int partitionId;
        private final List<LogSegment> segments;
        private final AtomicLong highWatermark;
        
        // 追加消息到分区
        public void append(RecordBatch batch) {
            LogSegment activeSegment = getActiveSegment();
            if (activeSegment == null || activeSegment.isFull()) {
                activeSegment = createNewSegment();
            }
            
            activeSegment.append(batch);
            highWatermark.set(batch.getLastOffset());
        }
        
        // 读取消息
        public List<RecordBatch> read(long offset, int maxSize) {
            List<RecordBatch> batches = new ArrayList<>();
            long currentOffset = offset;
            
            for (LogSegment segment : segments) {
                if (segment.contains(currentOffset)) {
                    RecordBatch batch = segment.read(currentOffset, maxSize);
                    if (batch != null) {
                        batches.add(batch);
                        currentOffset = batch.getLastOffset() + 1;
                    }
                }
            }
            
            return batches;
        }
    }
    
    // 3. 索引管理
    public class IndexManager {
        private final Map<Long, Long> offsetIndex;  // 偏移量索引
        private final Map<Long, Long> timeIndex;    // 时间索引
        
        // 更新偏移量索引
        public void updateOffsetIndex(long offset, long position) {
            offsetIndex.put(offset, position);
        }
        
        // 更新时间索引
        public void updateTimeIndex(long timestamp, long offset) {
            timeIndex.put(timestamp, offset);
        }
        
        // 根据偏移量查找位置
        public long getPosition(long offset) {
            return offsetIndex.get(offset);
        }
        
        // 根据时间查找偏移量
        public long getOffsetByTime(long timestamp) {
            return timeIndex.get(timestamp);
        }
    }
}
```

#### 2.1.3 Kafka副本机制深度解析

**副本机制实现**：
```java
// Kafka副本机制实现
public class KafkaReplication {
    
    // 1. 副本状态管理
    public enum ReplicaState {
        OFFLINE,        // 离线
        ONLINE,         // 在线
        RECOVERING,     // 恢复中
        LEADER,         // 主副本
        FOLLOWER        // 从副本
    }
    
    // 2. 副本管理器
    public class ReplicaManager {
        private final Map<TopicPartition, Replica> replicas;
        private final Map<TopicPartition, LeaderAndIsr> leaders;
        
        // 处理副本同步
        public void handleReplicaSync(TopicPartition partition, long offset) {
            Replica replica = replicas.get(partition);
            if (replica != null) {
                replica.updateHighWatermark(offset);
                
                // 检查是否需要进行ISR更新
                if (shouldUpdateIsr(partition)) {
                    updateIsr(partition);
                }
            }
        }
        
        // 更新ISR（In-Sync Replicas）
        private void updateIsr(TopicPartition partition) {
            LeaderAndIsr leaderAndIsr = leaders.get(partition);
            List<Replica> isr = getInSyncReplicas(partition);
            
            if (isr.size() != leaderAndIsr.getIsr().size()) {
                LeaderAndIsr newLeaderAndIsr = new LeaderAndIsr(
                    leaderAndIsr.getLeader(),
                    leaderAndIsr.getLeaderEpoch() + 1,
                    isr
                );
                leaders.put(partition, newLeaderAndIsr);
            }
        }
    }
    
    // 3. 副本同步机制
    public class ReplicaSync {
        private final ReplicaManager replicaManager;
        private final ScheduledExecutorService scheduler;
        
        // 启动副本同步
        public void startReplicaSync(TopicPartition partition) {
            scheduler.scheduleAtFixedRate(() -> {
                try {
                    syncReplica(partition);
                } catch (Exception e) {
                    log.error("Failed to sync replica for partition {}", partition, e);
                }
            }, 0, 100, TimeUnit.MILLISECONDS);
        }
        
        // 同步副本
        private void syncReplica(TopicPartition partition) {
            Replica replica = replicaManager.getReplica(partition);
            if (replica != null && replica.getState() == ReplicaState.FOLLOWER) {
                long leaderOffset = getLeaderOffset(partition);
                long followerOffset = replica.getHighWatermark();
                
                if (leaderOffset > followerOffset) {
                    // 从主副本同步数据
                    syncDataFromLeader(partition, followerOffset, leaderOffset);
                }
            }
        }
    }
}
```

### 2.2 Kafka Producer深度原理

#### 2.2.1 Producer核心实现

**Producer实现原理**：
```java
// Kafka Producer核心实现
public class KafkaProducerImpl {
    
    // 1. 消息发送器
    public class RecordSender {
        private final Map<TopicPartition, Deque<RecordBatch>> batches;
        private final Map<TopicPartition, Long> nextSequenceNumbers;
        private final int batchSize;
        private final long lingerMs;
        
        // 发送消息
        public Future<RecordMetadata> send(ProducerRecord record) {
            // 序列化消息
            byte[] serializedKey = serialize(record.key());
            byte[] serializedValue = serialize(record.value());
            
            // 计算分区
            int partition = partition(record.topic(), record.key());
            TopicPartition topicPartition = new TopicPartition(record.topic(), partition);
            
            // 添加到批次
            RecordBatch batch = getOrCreateBatch(topicPartition);
            batch.add(record, serializedKey, serializedValue);
            
            // 检查是否需要发送批次
            if (batch.isFull() || batch.isExpired()) {
                sendBatch(batch);
            }
            
            return batch.getFuture();
        }
        
        // 发送批次
        private void sendBatch(RecordBatch batch) {
            TopicPartition topicPartition = batch.getTopicPartition();
            
            // 构建发送请求
            ProduceRequest request = new ProduceRequest(
                topicPartition.topic(),
                topicPartition.partition(),
                batch.getRecords()
            );
            
            // 发送到Broker
            sendToBroker(request);
        }
    }
    
    // 2. 分区器实现
    public class DefaultPartitioner implements Partitioner {
        private final AtomicInteger counter = new AtomicInteger(0);
        
        @Override
        public int partition(String topic, Object key, byte[] keyBytes, 
                           Object value, byte[] valueBytes, Cluster cluster) {
            List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
            int numPartitions = partitions.size();
            
            if (keyBytes == null) {
                // 轮询分区
                return counter.getAndIncrement() % numPartitions;
            } else {
                // 基于key的哈希分区
                return Math.abs(Utils.murmur2(keyBytes)) % numPartitions;
            }
        }
    }
    
    // 3. 确认机制
    public class AcknowledgmentHandler {
        private final Map<TopicPartition, Long> acks;
        private final int requiredAcks;
        
        public void handleAcknowledgment(TopicPartition partition, long offset) {
            acks.put(partition, offset);
            
            // 检查是否满足确认要求
            if (acks.size() >= requiredAcks) {
                notifyCompletion();
            }
        }
        
        public void handleError(TopicPartition partition, Exception error) {
            // 处理错误，可能重试或失败
            handleRetry(partition, error);
        }
    }
}
```

#### 2.2.2 Producer性能优化

**性能优化实现**：
```java
// Producer性能优化
public class ProducerOptimization {
    
    // 1. 批量发送优化
    public class BatchOptimization {
        private final Map<TopicPartition, RecordBatch> batches;
        private final int batchSize;
        private final long lingerMs;
        
        public void optimizeBatch(TopicPartition partition, ProducerRecord record) {
            RecordBatch batch = batches.get(partition);
            if (batch == null) {
                batch = new RecordBatch(partition, batchSize, lingerMs);
                batches.put(partition, batch);
            }
            
            batch.add(record);
            
            // 检查是否需要发送
            if (batch.isFull() || batch.isExpired()) {
                sendBatch(batch);
                batches.remove(partition);
            }
        }
    }
    
    // 2. 压缩优化
    public class CompressionOptimization {
        private final CompressionType compressionType;
        private final int compressionLevel;
        
        public byte[] compress(byte[] data) {
            switch (compressionType) {
                case GZIP:
                    return gzipCompress(data);
                case SNAPPY:
                    return snappyCompress(data);
                case LZ4:
                    return lz4Compress(data);
                case ZSTD:
                    return zstdCompress(data);
                default:
                    return data;
            }
        }
        
        private byte[] gzipCompress(byte[] data) {
            try (ByteArrayOutputStream baos = new ByteArrayOutputStream();
                 GZIPOutputStream gzos = new GZIPOutputStream(baos)) {
                gzos.write(data);
                gzos.finish();
                return baos.toByteArray();
            } catch (IOException e) {
                throw new RuntimeException("Compression failed", e);
            }
        }
    }
    
    // 3. 异步发送优化
    public class AsyncSendOptimization {
        private final ExecutorService executor;
        private final BlockingQueue<SendTask> sendQueue;
        
        public void asyncSend(ProducerRecord record) {
            SendTask task = new SendTask(record);
            sendQueue.offer(task);
            
            // 异步处理
            executor.submit(() -> {
                try {
                    processSendTask(task);
                } catch (Exception e) {
                    handleSendError(task, e);
                }
            });
        }
        
        private void processSendTask(SendTask task) {
            // 处理发送任务
            sendRecord(task.getRecord());
        }
    }
}
```

### 2.3 Kafka Consumer深度原理

#### 2.3.1 Consumer核心实现

**Consumer实现原理**：
```java
// Kafka Consumer核心实现
public class KafkaConsumerImpl {
    
    // 1. 消费者组管理
    public class ConsumerGroupManager {
        private final String groupId;
        private final Map<String, ConsumerMetadata> consumers;
        private final Map<TopicPartition, ConsumerAssignment> assignments;
        
        // 加入消费者组
        public void joinGroup(String consumerId) {
            JoinGroupRequest request = new JoinGroupRequest(
                groupId,
                consumerId,
                getSubscribedTopics()
            );
            
            JoinGroupResponse response = sendJoinGroupRequest(request);
            
            if (response.isLeader()) {
                // 作为Leader分配分区
                assignPartitions(response.getMembers());
            } else {
                // 作为Follower接收分配
                receiveAssignment(response.getAssignment());
            }
        }
        
        // 分配分区
        private void assignPartitions(List<String> members) {
            Map<String, List<TopicPartition>> assignments = new HashMap<>();
            
            // 获取所有分区
            List<TopicPartition> allPartitions = getAllPartitions();
            
            // 轮询分配
            for (int i = 0; i < allPartitions.size(); i++) {
                String member = members.get(i % members.size());
                assignments.computeIfAbsent(member, k -> new ArrayList<>())
                          .add(allPartitions.get(i));
            }
            
            // 发送分配结果
            sendAssignment(assignments);
        }
    }
    
    // 2. 偏移量管理
    public class OffsetManager {
        private final Map<TopicPartition, Long> offsets;
        private final OffsetCommitCallback commitCallback;
        
        // 提交偏移量
        public void commitOffsets(Map<TopicPartition, Long> offsets) {
            CommitOffsetRequest request = new CommitOffsetRequest(
                groupId,
                offsets
            );
            
            CommitOffsetResponse response = sendCommitRequest(request);
            
            if (response.isSuccess()) {
                // 更新本地偏移量
                updateLocalOffsets(offsets);
            } else {
                // 处理提交失败
                handleCommitFailure(offsets);
            }
        }
        
        // 获取偏移量
        public long getOffset(TopicPartition partition) {
            return offsets.getOrDefault(partition, 0L);
        }
        
        // 更新偏移量
        public void updateOffset(TopicPartition partition, long offset) {
            offsets.put(partition, offset);
        }
    }
    
    // 3. 消息拉取
    public class MessageFetcher {
        private final Map<TopicPartition, FetchSession> fetchSessions;
        private final int fetchSize;
        private final long fetchTimeout;
        
        // 拉取消息
        public List<ConsumerRecord> fetchMessages(Map<TopicPartition, Long> offsets) {
            List<FetchRequest> requests = new ArrayList<>();
            
            for (Map.Entry<TopicPartition, Long> entry : offsets.entrySet()) {
                TopicPartition partition = entry.getKey();
                Long offset = entry.getValue();
                
                FetchRequest request = new FetchRequest(
                    partition,
                    offset,
                    fetchSize
                );
                requests.add(request);
            }
            
            // 发送拉取请求
            List<FetchResponse> responses = sendFetchRequests(requests);
            
            // 处理响应
            List<ConsumerRecord> records = new ArrayList<>();
            for (FetchResponse response : responses) {
                records.addAll(response.getRecords());
            }
            
            return records;
        }
    }
}
```

### 2.4 Kafka性能优化深度解析

#### 2.4.1 存储优化

**存储优化实现**：
```java
// Kafka存储优化
public class KafkaStorageOptimization {
    
    // 1. 日志段优化
    public class LogSegmentOptimization {
        private final int segmentSize;
        private final long segmentMs;
        private final boolean enableCompaction;
        
        // 日志段压缩
        public void compactLogSegment(LogSegment segment) {
            if (!enableCompaction) return;
            
            Map<String, Record> latestRecords = new HashMap<>();
            
            // 收集最新记录
            for (Record record : segment.getRecords()) {
                String key = record.getKey();
                if (key != null) {
                    latestRecords.put(key, record);
                }
            }
            
            // 创建压缩后的日志段
            LogSegment compactedSegment = createCompactedSegment(latestRecords.values());
            replaceSegment(segment, compactedSegment);
        }
    }
    
    // 2. 索引优化
    public class IndexOptimization {
        private final int indexInterval;
        private final boolean enableTimeIndex;
        
        // 稀疏索引
        public void buildSparseIndex(LogSegment segment) {
            long offset = segment.getBaseOffset();
            long position = 0;
            
            for (Record record : segment.getRecords()) {
                if (offset % indexInterval == 0) {
                    // 添加索引项
                    addIndexEntry(offset, position);
                }
                offset++;
                position += record.size();
            }
        }
        
        // 时间索引
        public void buildTimeIndex(LogSegment segment) {
            long offset = segment.getBaseOffset();
            
            for (Record record : segment.getRecords()) {
                long timestamp = record.getTimestamp();
                if (timestamp % 1000 == 0) { // 每秒一个索引项
                    addTimeIndexEntry(timestamp, offset);
                }
                offset++;
            }
        }
    }
    
    // 3. 缓存优化
    public class CacheOptimization {
        private final Map<String, ByteBuffer> pageCache;
        private final int cacheSize;
        
        // 页缓存管理
        public ByteBuffer getFromCache(String key) {
            return pageCache.get(key);
        }
        
        public void putToCache(String key, ByteBuffer buffer) {
            if (pageCache.size() >= cacheSize) {
                // 清理最旧的缓存
                evictOldestCache();
            }
            pageCache.put(key, buffer);
        }
        
        // 预读优化
        public void prefetchData(String key, int size) {
            // 异步预读数据
            CompletableFuture.runAsync(() -> {
                ByteBuffer data = readData(key, size);
                putToCache(key, data);
            });
        }
    }
}
```

#### 2.4.2 网络优化

**网络优化实现**：
```java
// Kafka网络优化
public class KafkaNetworkOptimization {
    
    // 1. 连接池管理
    public class ConnectionPool {
        private final Map<String, List<Connection>> connections;
        private final int maxConnectionsPerBroker;
        private final long connectionTimeout;
        
        // 获取连接
        public Connection getConnection(String brokerId) {
            List<Connection> brokerConnections = connections.get(brokerId);
            
            if (brokerConnections == null || brokerConnections.isEmpty()) {
                return createNewConnection(brokerId);
            }
            
            // 返回可用连接
            return brokerConnections.stream()
                .filter(Connection::isAvailable)
                .findFirst()
                .orElse(createNewConnection(brokerId));
        }
        
        // 创建新连接
        private Connection createNewConnection(String brokerId) {
            Connection connection = new Connection(brokerId);
            connections.computeIfAbsent(brokerId, k -> new ArrayList<>())
                      .add(connection);
            return connection;
        }
    }
    
    // 2. 请求批处理
    public class RequestBatching {
        private final Map<String, List<Request>> pendingRequests;
        private final int batchSize;
        private final long batchTimeout;
        
        // 批量发送请求
        public void batchSend(String brokerId, Request request) {
            pendingRequests.computeIfAbsent(brokerId, k -> new ArrayList<>())
                          .add(request);
            
            List<Request> requests = pendingRequests.get(brokerId);
            if (requests.size() >= batchSize) {
                sendBatch(brokerId, requests);
                requests.clear();
            }
        }
        
        // 发送批次
        private void sendBatch(String brokerId, List<Request> requests) {
            BatchRequest batchRequest = new BatchRequest(requests);
            sendToBroker(brokerId, batchRequest);
        }
    }
    
    // 3. 压缩传输
    public class CompressionTransport {
        private final CompressionType compressionType;
        private final int compressionLevel;
        
        // 压缩请求
        public CompressedRequest compressRequest(Request request) {
            byte[] data = request.serialize();
            byte[] compressedData = compress(data);
            
            return new CompressedRequest(
                request.getType(),
                compressedData,
                compressionType
            );
        }
        
        // 解压响应
        public Response decompressResponse(CompressedResponse compressedResponse) {
            byte[] compressedData = compressedResponse.getData();
            byte[] data = decompress(compressedData);
            
            return Response.deserialize(data);
        }
    }
}
```

## 三、RocketMQ深度原理

### 1.3 Consumer

```java
@Component
public class KafkaConsumerService {
    
    @KafkaListener(topics = "my-topic", groupId = "my-group")
    public void consume(String message) {
        log.info("Consumed: {}", message);
    }
}
```

### 1.4 核心特性

**1. 高吞吐**：
- 顺序写磁盘
- 页缓存
- 零拷贝

**2. 持久化**：
- 消息持久化到磁盘
- 多副本机制

**3. 分区**：
- 水平扩展
- 并行消费

## 二、RocketMQ

### 2.1 核心概念

- **NameServer**：路由中心
- **Broker**：存储消息
- **Producer**：生产者
- **Consumer**：消费者

### 2.2 消息类型

**1. 普通消息**：
```java
@Component
public class RocketMQProducer {
    
    @Autowired
    private RocketMQTemplate rocketMQTemplate;
    
    public void send(String topic, String message) {
        rocketMQTemplate.convertAndSend(topic, message);
    }
}
```

**2. 顺序消息**：
```java
public void sendOrderly(String topic, String message, String orderKey) {
    rocketMQTemplate.syncSendOrderly(topic, message, orderKey);
}
```

**3. 事务消息**：
```java
@Component
@RocketMQTransactionListener
public class TransactionListenerImpl implements RocketMQLocalTransactionListener {
    
    @Override
    public RocketMQLocalTransactionState executeLocalTransaction(Message msg, Object arg) {
        // 执行本地事务
        try {
            orderService.createOrder((Order) arg);
            return RocketMQLocalTransactionState.COMMIT;
        } catch (Exception e) {
            return RocketMQLocalTransactionState.ROLLBACK;
        }
    }
    
    @Override
    public RocketMQLocalTransactionState checkLocalTransaction(Message msg) {
        // 事务回查
        return RocketMQLocalTransactionState.COMMIT;
    }
}
```

## 三、消息队列对比

| 特性 | Kafka | RocketMQ | RabbitMQ |
|------|-------|----------|----------|
| 吞吐量 | 极高 | 高 | 中 |
| 延迟 | ms级 | ms级 | us级 |
| 可靠性 | 高 | 极高 | 高 |
| 消息有序 | 分区有序 | 支持全局有序 | 队列有序 |
| 事务消息 | 不支持 | 支持 | 支持 |

## 四、高频面试题

### Q1：Kafka如何保证消息不丢失？

1. **Producer**：设置acks=all
2. **Broker**：设置副本数>1
3. **Consumer**：手动提交offset

### Q2：Kafka如何保证消息顺序？

- 单分区内有序
- 使用相同key路由到同一分区

### Q3：RocketMQ事务消息原理？

1. 发送半消息（half message）
2. 执行本地事务
3. 提交或回滚
4. 事务回查

---

**关键字**：Kafka、RocketMQ、消息队列、事务消息、顺序消息

