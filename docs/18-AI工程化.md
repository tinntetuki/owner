# AI工程化深度解析

## 目录
- [一、MLOps](#一mlops)
- [二、模型服务化](#二模型服务化)
- [三、A/B测试](#三ab测试)
- [四、特征工程](#四特征工程)

## 一、MLOps

### 1.1 ML Pipeline

```python
from airflow import DAG
from airflow.operators.python import PythonOperator

dag = DAG('ml_pipeline', schedule_interval='@daily')

def extract_data():
    # 数据提取
    pass

def train_model():
    # 模型训练
    pass

def evaluate_model():
    # 模型评估
    pass

def deploy_model():
    # 模型部署
    pass

t1 = PythonOperator(task_id='extract', python_callable=extract_data, dag=dag)
t2 = PythonOperator(task_id='train', python_callable=train_model, dag=dag)
t3 = PythonOperator(task_id='evaluate', python_callable=evaluate_model, dag=dag)
t4 = PythonOperator(task_id='deploy', python_callable=deploy_model, dag=dag)

t1 >> t2 >> t3 >> t4
```

### 1.2 实验跟踪

```python
import mlflow

with mlflow.start_run():
    # 训练模型
    model = train_model(params)
    
    # 记录参数
    mlflow.log_params(params)
    
    # 记录指标
    mlflow.log_metrics({
        'accuracy': accuracy,
        'loss': loss
    })
    
    # 保存模型
    mlflow.sklearn.log_model(model, "model")
```

## 二、模型服务化

### 2.1 TensorFlow Serving

```bash
# 导出模型
python
import tensorflow as tf

model.save('/models/my_model/1')

# 启动服务
docker run -p 8501:8501 \
  --mount type=bind,source=/models/my_model,target=/models/my_model \
  -e MODEL_NAME=my_model \
  -t tensorflow/serving
```

**调用服务**：
```python
import requests
import json

data = {
    "instances": [[1.0, 2.0, 3.0]]
}

response = requests.post(
    'http://localhost:8501/v1/models/my_model:predict',
    data=json.dumps(data)
)

predictions = response.json()['predictions']
```

### 2.2 ONNX Runtime

```python
import onnxruntime as ort

# 加载模型
session = ort.InferenceSession("model.onnx")

# 推理
inputs = {session.get_inputs()[0].name: input_data}
outputs = session.run(None, inputs)
```

## 三、A/B测试

### 3.1 流量分配

```python
import hashlib

def assign_bucket(user_id, experiment_id):
    hash_key = f"{experiment_id}:{user_id}"
    hash_value = int(hashlib.md5(hash_key.encode()).hexdigest(), 16)
    return hash_value % 100

def get_version(user_id):
    bucket = assign_bucket(user_id, 'exp_001')
    if bucket < 50:
        return 'A'  # 对照组
    else:
        return 'B'  # 实验组
```

### 3.2 指标计算

```python
class ABTestAnalyzer:
    def calculate_metrics(self, group_a, group_b):
        # 转化率
        cvr_a = sum(group_a['converted']) / len(group_a)
        cvr_b = sum(group_b['converted']) / len(group_b)
        
        # 显著性检验
        from scipy.stats import chi2_contingency
        
        obs = [[sum(group_a['converted']), len(group_a) - sum(group_a['converted'])],
               [sum(group_b['converted']), len(group_b) - sum(group_b['converted'])]]
        
        chi2, p_value, dof, expected = chi2_contingency(obs)
        
        return {
            'cvr_a': cvr_a,
            'cvr_b': cvr_b,
            'lift': (cvr_b - cvr_a) / cvr_a,
            'p_value': p_value,
            'significant': p_value < 0.05
        }
```

## 四、特征工程

### 4.1 特征存储

```python
from feast import FeatureStore

# 定义特征
store = FeatureStore(repo_path=".")

# 获取特征
features = store.get_online_features(
    features=[
        'user_features:age',
        'user_features:city',
        'item_features:category'
    ],
    entity_rows=[
        {"user_id": 1001, "item_id": 2001}
    ]
).to_dict()
```

### 4.2 特征转换

```python
from sklearn.preprocessing import StandardScaler, LabelEncoder

class FeatureTransformer:
    def __init__(self):
        self.scalers = {}
        self.encoders = {}
    
    def fit_transform(self, df):
        # 数值特征标准化
        numeric_features = ['age', 'price']
        for col in numeric_features:
            self.scalers[col] = StandardScaler()
            df[col] = self.scalers[col].fit_transform(df[[col]])
        
        # 类别特征编码
        categorical_features = ['gender', 'city']
        for col in categorical_features:
            self.encoders[col] = LabelEncoder()
            df[col] = self.encoders[col].fit_transform(df[col])
        
        return df
```

## 五、MLOps完整流程

### 5.1 数据管理

**数据版本控制**：
```python
import dvc.api
import pandas as pd
from typing import Dict, List, Optional
import logging

class DataVersionControl:
    def __init__(self, repo_path: str = "."):
        self.repo_path = repo_path
        self.logger = logging.getLogger(__name__)
    
    def track_data(self, data_path: str, metadata: Dict) -> str:
        """跟踪数据版本"""
        import dvc.api
        
        # 添加数据到DVC
        dvc.api.add(data_path)
        
        # 记录元数据
        metadata_file = f"{data_path}.meta"
        with open(metadata_file, 'w') as f:
            import json
            json.dump(metadata, f)
        
        # 提交到Git
        import subprocess
        subprocess.run(['git', 'add', data_path, metadata_file])
        subprocess.run(['git', 'commit', '-m', f'Add data: {data_path}'])
        
        return subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip()
    
    def get_data(self, data_path: str, version: Optional[str] = None) -> pd.DataFrame:
        """获取指定版本的数据"""
        if version:
            # 切换到指定版本
            import subprocess
            subprocess.run(['git', 'checkout', version])
        
        # 使用DVC获取数据
        import dvc.api
        data_url = dvc.api.get_url(data_path, repo=self.repo_path)
        
        return pd.read_csv(data_url)
    
    def list_data_versions(self, data_path: str) -> List[Dict]:
        """列出数据的所有版本"""
        import subprocess
        
        # 获取包含该文件的提交历史
        commits = subprocess.check_output([
            'git', 'log', '--oneline', '--follow', '--', data_path
        ]).decode().strip().split('\n')
        
        versions = []
        for commit in commits:
            if commit:
                commit_hash = commit.split()[0]
                commit_msg = ' '.join(commit.split()[1:])
                versions.append({
                    'hash': commit_hash,
                    'message': commit_msg
                })
        
        return versions
```

**数据质量监控**：
```python
import great_expectations as ge
from typing import Dict, List
import pandas as pd

class DataQualityMonitor:
    def __init__(self):
        self.expectations = {}
    
    def create_expectations(self, df: pd.DataFrame, config: Dict) -> None:
        """创建数据质量期望"""
        ge_df = ge.from_pandas(df)
        
        # 数据完整性检查
        if 'completeness' in config:
            for col in config['completeness']['columns']:
                ge_df.expect_column_values_to_not_be_null(col)
        
        # 数据类型检查
        if 'types' in config:
            for col, expected_type in config['types'].items():
                if expected_type == 'numeric':
                    ge_df.expect_column_values_to_be_of_type(col, 'float64')
                elif expected_type == 'string':
                    ge_df.expect_column_values_to_be_of_type(col, 'object')
        
        # 数值范围检查
        if 'ranges' in config:
            for col, range_config in config['ranges'].items():
                if 'min' in range_config:
                    ge_df.expect_column_values_to_be_between(
                        col, min_value=range_config['min']
                    )
                if 'max' in range_config:
                    ge_df.expect_column_values_to_be_between(
                        col, max_value=range_config['max']
                    )
        
        # 唯一性检查
        if 'uniqueness' in config:
            for col in config['uniqueness']:
                ge_df.expect_column_values_to_be_unique(col)
        
        # 保存期望
        self.expectations[config['name']] = ge_df.get_expectation_suite()
    
    def validate_data(self, df: pd.DataFrame, suite_name: str) -> Dict:
        """验证数据质量"""
        ge_df = ge.from_pandas(df)
        suite = self.expectations[suite_name]
        
        validation_result = ge_df.validate(expectation_suite=suite)
        
        return {
            'success': validation_result.success,
            'statistics': validation_result.statistics,
            'results': validation_result.results
        }
    
    def generate_report(self, validation_result: Dict) -> str:
        """生成数据质量报告"""
        report = "Data Quality Report\n"
        report += "=" * 50 + "\n"
        
        if validation_result['success']:
            report += "✅ All expectations passed!\n"
        else:
            report += "❌ Some expectations failed!\n"
        
        report += f"\nStatistics:\n"
        for key, value in validation_result['statistics'].items():
            report += f"  {key}: {value}\n"
        
        report += f"\nDetailed Results:\n"
        for result in validation_result['results']:
            status = "✅" if result.success else "❌"
            report += f"  {status} {result.expectation_type}: {result.result}\n"
        
        return report
```

### 5.2 模型训练与实验管理

**分布式训练**：
```python
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import os
from typing import Dict, Any

class DistributedTraining:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.rank = int(os.environ.get('RANK', 0))
        self.world_size = int(os.environ.get('WORLD_SIZE', 1))
        self.local_rank = int(os.environ.get('LOCAL_RANK', 0))
        
        # 初始化分布式环境
        self.init_distributed()
    
    def init_distributed(self):
        """初始化分布式训练环境"""
        if self.world_size > 1:
            dist.init_process_group(
                backend='nccl' if torch.cuda.is_available() else 'gloo',
                init_method='env://',
                world_size=self.world_size,
                rank=self.rank
            )
    
    def setup_model(self, model: torch.nn.Module) -> torch.nn.Module:
        """设置分布式模型"""
        if torch.cuda.is_available():
            model = model.cuda(self.local_rank)
        
        if self.world_size > 1:
            model = DDP(model, device_ids=[self.local_rank])
        
        return model
    
    def setup_dataloader(self, dataset, batch_size: int):
        """设置分布式数据加载器"""
        sampler = None
        if self.world_size > 1:
            sampler = DistributedSampler(
                dataset,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=True
            )
        
        return torch.utils.data.DataLoader(
            dataset,
            batch_size=batch_size,
            sampler=sampler,
            num_workers=self.config.get('num_workers', 4),
            pin_memory=torch.cuda.is_available()
        )
    
    def train_epoch(self, model, dataloader, optimizer, criterion):
        """训练一个epoch"""
        model.train()
        total_loss = 0
        
        for batch_idx, (data, target) in enumerate(dataloader):
            if torch.cuda.is_available():
                data, target = data.cuda(self.local_rank), target.cuda(self.local_rank)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            if batch_idx % self.config.get('log_interval', 100) == 0:
                print(f'Rank {self.rank}, Batch {batch_idx}, Loss: {loss.item():.6f}')
        
        return total_loss / len(dataloader)
    
    def cleanup(self):
        """清理分布式环境"""
        if self.world_size > 1:
            dist.destroy_process_group()
```

**超参数优化**：
```python
import optuna
from typing import Dict, Any, Callable
import logging

class HyperparameterOptimizer:
    def __init__(self, study_name: str, storage_url: str = None):
        self.study_name = study_name
        self.storage_url = storage_url
        self.logger = logging.getLogger(__name__)
    
    def create_study(self, direction: str = 'minimize') -> optuna.Study:
        """创建优化研究"""
        if self.storage_url:
            study = optuna.create_study(
                study_name=self.study_name,
                storage=self.storage_url,
                load_if_exists=True,
                direction=direction
            )
        else:
            study = optuna.create_study(direction=direction)
        
        return study
    
    def optimize(self, objective_func: Callable, n_trials: int = 100) -> Dict[str, Any]:
        """执行超参数优化"""
        study = self.create_study()
        
        study.optimize(objective_func, n_trials=n_trials)
        
        best_params = study.best_params
        best_value = study.best_value
        
        self.logger.info(f"Best parameters: {best_params}")
        self.logger.info(f"Best value: {best_value}")
        
        return {
            'best_params': best_params,
            'best_value': best_value,
            'study': study
        }
    
    def suggest_parameters(self, trial: optuna.Trial, config: Dict[str, Any]) -> Dict[str, Any]:
        """建议超参数"""
        params = {}
        
        for param_name, param_config in config.items():
            param_type = param_config['type']
            
            if param_type == 'int':
                params[param_name] = trial.suggest_int(
                    param_name,
                    param_config['low'],
                    param_config['high'],
                    step=param_config.get('step', 1)
                )
            elif param_type == 'float':
                params[param_name] = trial.suggest_float(
                    param_name,
                    param_config['low'],
                    param_config['high'],
                    step=param_config.get('step', None),
                    log=param_config.get('log', False)
                )
            elif param_type == 'categorical':
                params[param_name] = trial.suggest_categorical(
                    param_name,
                    param_config['choices']
                )
            elif param_type == 'uniform':
                params[param_name] = trial.suggest_uniform(
                    param_name,
                    param_config['low'],
                    param_config['high']
                )
            elif param_type == 'loguniform':
                params[param_name] = trial.suggest_loguniform(
                    param_name,
                    param_config['low'],
                    param_config['high']
                )
        
        return params
    
    def plot_optimization_history(self, study: optuna.Study):
        """绘制优化历史"""
        import matplotlib.pyplot as plt
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # 优化历史
        optuna.visualization.matplotlib.plot_optimization_history(study, ax=ax1)
        
        # 参数重要性
        optuna.visualization.matplotlib.plot_param_importances(study, ax=ax2)
        
        plt.tight_layout()
        plt.show()
```

### 5.3 模型版本管理

**模型注册与版本控制**：
```python
import mlflow
import mlflow.sklearn
import mlflow.pytorch
import mlflow.tensorflow
from typing import Dict, Any, Optional, List
import os
import shutil
import logging

class ModelRegistry:
    def __init__(self, tracking_uri: str = "sqlite:///mlflow.db"):
        self.tracking_uri = tracking_uri
        mlflow.set_tracking_uri(tracking_uri)
        self.logger = logging.getLogger(__name__)
    
    def register_model(self, model, model_name: str, 
                      model_type: str = "sklearn",
                      metadata: Dict[str, Any] = None) -> str:
        """注册模型"""
        with mlflow.start_run() as run:
            # 记录模型
            if model_type == "sklearn":
                mlflow.sklearn.log_model(model, "model")
            elif model_type == "pytorch":
                mlflow.pytorch.log_model(model, "model")
            elif model_type == "tensorflow":
                mlflow.tensorflow.log_model(model, "model")
            
            # 记录元数据
            if metadata:
                mlflow.log_params(metadata)
            
            # 注册模型
            model_uri = f"runs:/{run.info.run_id}/model"
            model_version = mlflow.register_model(model_uri, model_name)
            
            self.logger.info(f"Registered model: {model_name}, version: {model_version.version}")
            
            return model_version.version
    
    def get_model(self, model_name: str, version: Optional[str] = None, stage: Optional[str] = None):
        """获取模型"""
        if stage:
            model_uri = f"models:/{model_name}/{stage}"
        elif version:
            model_uri = f"models:/{model_name}/{version}"
        else:
            model_uri = f"models:/{model_name}/latest"
        
        return mlflow.pyfunc.load_model(model_uri)
    
    def transition_model_stage(self, model_name: str, version: str, 
                             stage: str, archive_existing_versions: bool = False):
        """转换模型阶段"""
        client = mlflow.tracking.MlflowClient()
        
        client.transition_model_version_stage(
            name=model_name,
            version=version,
            stage=stage,
            archive_existing_versions=archive_existing_versions
        )
        
        self.logger.info(f"Transitioned {model_name} version {version} to {stage}")
    
    def list_models(self) -> List[Dict[str, Any]]:
        """列出所有模型"""
        client = mlflow.tracking.MlflowClient()
        models = client.search_registered_models()
        
        model_list = []
        for model in models:
            model_list.append({
                'name': model.name,
                'latest_versions': [
                    {
                        'version': version.version,
                        'stage': version.current_stage,
                        'creation_timestamp': version.creation_timestamp
                    }
                    for version in model.latest_versions
                ]
            })
        
        return model_list
    
    def compare_models(self, model_name: str, versions: List[str]) -> Dict[str, Any]:
        """比较模型版本"""
        client = mlflow.tracking.MlflowClient()
        
        comparison = {}
        for version in versions:
            run_id = client.get_model_version(model_name, version).run_id
            run = client.get_run(run_id)
            
            comparison[version] = {
                'metrics': run.data.metrics,
                'params': run.data.params,
                'tags': run.data.tags
            }
        
        return comparison
    
    def delete_model(self, model_name: str, version: Optional[str] = None):
        """删除模型"""
        client = mlflow.tracking.MlflowClient()
        
        if version:
            client.delete_model_version(model_name, version)
            self.logger.info(f"Deleted model {model_name} version {version}")
        else:
            client.delete_registered_model(model_name)
            self.logger.info(f"Deleted model {model_name}")
```

## 六、模型服务化架构

### 6.1 模型服务化平台

**TensorFlow Serving生产级部署**：
```python
import tensorflow as tf
import grpc
from tensorflow_serving.apis import predict_pb2
from tensorflow_serving.apis import prediction_service_pb2_grpc
import numpy as np
from typing import Dict, Any, List
import logging

class TensorFlowServingClient:
    def __init__(self, host: str = 'localhost', port: int = 8500):
        self.host = host
        self.port = port
        self.channel = grpc.insecure_channel(f'{host}:{port}')
        self.stub = prediction_service_pb2_grpc.PredictionServiceStub(self.channel)
        self.logger = logging.getLogger(__name__)
    
    def predict(self, model_name: str, model_version: str, 
               inputs: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """发送预测请求"""
        request = predict_pb2.PredictRequest()
        request.model_spec.name = model_name
        request.model_spec.version.value = int(model_version)
        
        # 添加输入数据
        for input_name, input_data in inputs.items():
            tensor_proto = tf.make_tensor_proto(input_data)
            request.inputs[input_name].CopyFrom(tensor_proto)
        
        try:
            response = self.stub.Predict(request, timeout=10.0)
            
            # 解析响应
            outputs = {}
            for output_name, tensor_proto in response.outputs.items():
                outputs[output_name] = tf.make_ndarray(tensor_proto)
            
            return outputs
        
        except grpc.RpcError as e:
            self.logger.error(f"RPC error: {e}")
            raise
    
    def get_model_metadata(self, model_name: str, model_version: str = None) -> Dict[str, Any]:
        """获取模型元数据"""
        from tensorflow_serving.apis import get_model_metadata_pb2
        
        request = get_model_metadata_pb2.GetModelMetadataRequest()
        request.model_spec.name = model_name
        if model_version:
            request.model_spec.version.value = int(model_version)
        
        try:
            response = self.stub.GetModelMetadata(request, timeout=10.0)
            
            metadata = {}
            for key, value in response.metadata.items():
                metadata[key] = value
            
            return metadata
        
        except grpc.RpcError as e:
            self.logger.error(f"RPC error: {e}")
            raise
    
    def health_check(self) -> bool:
        """健康检查"""
        try:
            # 尝试获取模型列表
            self.get_model_metadata("dummy_model")
            return True
        except:
            return False
```

**模型服务化API网关**：
```python
from flask import Flask, request, jsonify
import requests
import json
from typing import Dict, Any, List
import logging
import time
from functools import wraps

class ModelServiceGateway:
    def __init__(self):
        self.app = Flask(__name__)
        self.model_services = {}
        self.logger = logging.getLogger(__name__)
        
        # 注册路由
        self.register_routes()
    
    def register_model_service(self, model_name: str, service_url: str, 
                             model_version: str = "latest"):
        """注册模型服务"""
        self.model_services[model_name] = {
            'url': service_url,
            'version': model_version,
            'last_health_check': time.time(),
            'healthy': True
        }
    
    def health_check_decorator(self, f):
        """健康检查装饰器"""
        @wraps(f)
        def decorated_function(*args, **kwargs):
            model_name = kwargs.get('model_name')
            if model_name and model_name in self.model_services:
                service = self.model_services[model_name]
                
                # 检查服务健康状态
                if not self.check_service_health(model_name):
                    return jsonify({'error': 'Model service unavailable'}), 503
            
            return f(*args, **kwargs)
        return decorated_function
    
    def check_service_health(self, model_name: str) -> bool:
        """检查服务健康状态"""
        service = self.model_services[model_name]
        
        # 每30秒检查一次
        if time.time() - service['last_health_check'] > 30:
            try:
                response = requests.get(f"{service['url']}/health", timeout=5)
                service['healthy'] = response.status_code == 200
                service['last_health_check'] = time.time()
            except:
                service['healthy'] = False
                service['last_health_check'] = time.time()
        
        return service['healthy']
    
    def register_routes(self):
        """注册API路由"""
        
        @self.app.route('/predict/<model_name>', methods=['POST'])
        @self.health_check_decorator
        def predict(model_name):
            """预测接口"""
            try:
                data = request.get_json()
                
                if model_name not in self.model_services:
                    return jsonify({'error': 'Model not found'}), 404
                
                service = self.model_services[model_name]
                
                # 转发请求到模型服务
                response = requests.post(
                    f"{service['url']}/predict",
                    json=data,
                    timeout=30
                )
                
                if response.status_code == 200:
                    return jsonify(response.json())
                else:
                    return jsonify({'error': 'Prediction failed'}), 500
            
            except Exception as e:
                self.logger.error(f"Prediction error: {e}")
                return jsonify({'error': 'Internal server error'}), 500
        
        @self.app.route('/models', methods=['GET'])
        def list_models():
            """列出所有模型"""
            models = []
            for model_name, service in self.model_services.items():
                models.append({
                    'name': model_name,
                    'version': service['version'],
                    'healthy': service['healthy']
                })
            
            return jsonify({'models': models})
        
        @self.app.route('/health', methods=['GET'])
        def health():
            """健康检查"""
            healthy_services = sum(1 for s in self.model_services.values() if s['healthy'])
            total_services = len(self.model_services)
            
            return jsonify({
                'status': 'healthy' if healthy_services == total_services else 'degraded',
                'services': f"{healthy_services}/{total_services}"
            })
    
    def run(self, host: str = '0.0.0.0', port: int = 5000, debug: bool = False):
        """启动服务"""
        self.app.run(host=host, port=port, debug=debug)
```

### 6.2 模型性能优化

**模型量化**：
```python
import torch
import torch.quantization as quantization
from torch.quantization import QuantStub, DeQuantStub
import numpy as np
from typing import Dict, Any

class ModelQuantizer:
    def __init__(self):
        self.quantized_model = None
    
    def quantize_model(self, model: torch.nn.Module, 
                      calibration_data: torch.utils.data.DataLoader,
                      quant_type: str = 'dynamic') -> torch.nn.Module:
        """量化模型"""
        
        if quant_type == 'dynamic':
            return self.dynamic_quantization(model)
        elif quant_type == 'static':
            return self.static_quantization(model, calibration_data)
        elif quant_type == 'qat':
            return self.quantization_aware_training(model)
        else:
            raise ValueError(f"Unsupported quantization type: {quant_type}")
    
    def dynamic_quantization(self, model: torch.nn.Module) -> torch.nn.Module:
        """动态量化"""
        # 设置量化配置
        model.eval()
        
        # 应用动态量化
        quantized_model = torch.quantization.quantize_dynamic(
            model,
            {torch.nn.Linear, torch.nn.Conv2d},
            dtype=torch.qint8
        )
        
        return quantized_model
    
    def static_quantization(self, model: torch.nn.Module, 
                          calibration_data: torch.utils.data.DataLoader) -> torch.nn.Module:
        """静态量化"""
        model.eval()
        
        # 添加量化存根
        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
        
        # 准备模型
        model_prepared = torch.quantization.prepare(model)
        
        # 校准
        with torch.no_grad():
            for data, _ in calibration_data:
                model_prepared(data)
        
        # 转换为量化模型
        quantized_model = torch.quantization.convert(model_prepared)
        
        return quantized_model
    
    def quantization_aware_training(self, model: torch.nn.Module) -> torch.nn.Module:
        """量化感知训练"""
        # 设置量化配置
        model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
        
        # 准备模型进行QAT
        model_prepared = torch.quantization.prepare_qat(model)
        
        return model_prepared
    
    def evaluate_quantization_impact(self, original_model: torch.nn.Module,
                                   quantized_model: torch.nn.Module,
                                   test_data: torch.utils.data.DataLoader) -> Dict[str, float]:
        """评估量化影响"""
        original_model.eval()
        quantized_model.eval()
        
        original_accuracies = []
        quantized_accuracies = []
        original_inference_times = []
        quantized_inference_times = []
        
        with torch.no_grad():
            for data, target in test_data:
                # 原始模型
                start_time = time.time()
                original_output = original_model(data)
                original_time = time.time() - start_time
                original_inference_times.append(original_time)
                
                # 量化模型
                start_time = time.time()
                quantized_output = quantized_model(data)
                quantized_time = time.time() - start_time
                quantized_inference_times.append(quantized_time)
                
                # 计算准确率
                original_pred = original_output.argmax(dim=1)
                quantized_pred = quantized_output.argmax(dim=1)
                
                original_acc = (original_pred == target).float().mean().item()
                quantized_acc = (quantized_pred == target).float().mean().item()
                
                original_accuracies.append(original_acc)
                quantized_accuracies.append(quantized_acc)
        
        return {
            'original_accuracy': np.mean(original_accuracies),
            'quantized_accuracy': np.mean(quantized_accuracies),
            'accuracy_drop': np.mean(original_accuracies) - np.mean(quantized_accuracies),
            'original_inference_time': np.mean(original_inference_times),
            'quantized_inference_time': np.mean(quantized_inference_times),
            'speedup': np.mean(original_inference_times) / np.mean(quantized_inference_times),
            'model_size_reduction': self.calculate_model_size_reduction(original_model, quantized_model)
        }
    
    def calculate_model_size_reduction(self, original_model: torch.nn.Module,
                                    quantized_model: torch.nn.Module) -> float:
        """计算模型大小减少"""
        import io
        
        # 保存原始模型
        original_buffer = io.BytesIO()
        torch.save(original_model.state_dict(), original_buffer)
        original_size = len(original_buffer.getvalue())
        
        # 保存量化模型
        quantized_buffer = io.BytesIO()
        torch.save(quantized_model.state_dict(), quantized_buffer)
        quantized_size = len(quantized_buffer.getvalue())
        
        return (original_size - quantized_size) / original_size
```

**模型剪枝**：
```python
import torch
import torch.nn.utils.prune as prune
from typing import Dict, Any, List
import numpy as np

class ModelPruner:
    def __init__(self):
        self.pruned_model = None
    
    def prune_model(self, model: torch.nn.Module, 
                   pruning_config: Dict[str, Any]) -> torch.nn.Module:
        """剪枝模型"""
        model.eval()
        
        # 应用剪枝
        for module_name, config in pruning_config.items():
            module = dict(model.named_modules())[module_name]
            
            if config['type'] == 'unstructured':
                prune.random_unstructured(
                    module,
                    name='weight',
                    amount=config['amount']
                )
            elif config['type'] == 'structured':
                prune.ln_structured(
                    module,
                    name='weight',
                    amount=config['amount'],
                    n=config.get('n', 2),
                    dim=config.get('dim', 0)
                )
            elif config['type'] == 'global':
                prune.global_unstructured(
                    [(module, 'weight')],
                    pruning_method=prune.L1Unstructured,
                    amount=config['amount']
                )
        
        # 移除剪枝参数
        prune.remove(model, 'weight')
        
        return model
    
    def iterative_pruning(self, model: torch.nn.Module,
                         train_loader: torch.utils.data.DataLoader,
                         test_loader: torch.utils.data.DataLoader,
                         pruning_schedule: List[float],
                         retrain_epochs: int = 5) -> torch.nn.Module:
        """迭代剪枝"""
        pruned_model = model
        
        for i, pruning_ratio in enumerate(pruning_schedule):
            print(f"Iteration {i+1}: Pruning {pruning_ratio*100}%")
            
            # 剪枝
            pruning_config = {
                'conv1': {'type': 'unstructured', 'amount': pruning_ratio},
                'conv2': {'type': 'unstructured', 'amount': pruning_ratio},
                'fc1': {'type': 'unstructured', 'amount': pruning_ratio},
                'fc2': {'type': 'unstructured', 'amount': pruning_ratio}
            }
            
            pruned_model = self.prune_model(pruned_model, pruning_config)
            
            # 重新训练
            if retrain_epochs > 0:
                pruned_model = self.retrain_model(
                    pruned_model, train_loader, retrain_epochs
                )
            
            # 评估
            accuracy = self.evaluate_model(pruned_model, test_loader)
            print(f"Accuracy after pruning: {accuracy:.4f}")
        
        return pruned_model
    
    def retrain_model(self, model: torch.nn.Module,
                     train_loader: torch.utils.data.DataLoader,
                     epochs: int) -> torch.nn.Module:
        """重新训练模型"""
        model.train()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = torch.nn.CrossEntropyLoss()
        
        for epoch in range(epochs):
            for data, target in train_loader:
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
        
        return model
    
    def evaluate_model(self, model: torch.nn.Module,
                      test_loader: torch.utils.data.DataLoader) -> float:
        """评估模型"""
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in test_loader:
                output = model(data)
                pred = output.argmax(dim=1)
                correct += (pred == target).sum().item()
                total += target.size(0)
        
        return correct / total
    
    def analyze_pruning_impact(self, original_model: torch.nn.Module,
                             pruned_model: torch.nn.Module) -> Dict[str, Any]:
        """分析剪枝影响"""
        # 计算参数数量
        original_params = sum(p.numel() for p in original_model.parameters())
        pruned_params = sum(p.numel() for p in pruned_model.parameters())
        
        # 计算非零参数数量
        original_nonzero = sum((p != 0).sum().item() for p in original_model.parameters())
        pruned_nonzero = sum((p != 0).sum().item() for p in pruned_model.parameters())
        
        return {
            'original_params': original_params,
            'pruned_params': pruned_params,
            'param_reduction': (original_params - pruned_params) / original_params,
            'original_nonzero': original_nonzero,
            'pruned_nonzero': pruned_nonzero,
            'sparsity': (original_nonzero - pruned_nonzero) / original_nonzero
        }
```

## 七、模型监控与告警

### 7.1 模型性能监控

**模型漂移检测**：
```python
import numpy as np
from scipy import stats
from typing import Dict, List, Any, Optional
import pandas as pd
from sklearn.metrics import roc_auc_score, accuracy_score
import logging

class ModelDriftDetector:
    def __init__(self, baseline_data: pd.DataFrame, baseline_predictions: np.ndarray):
        self.baseline_data = baseline_data
        self.baseline_predictions = baseline_predictions
        self.baseline_stats = self._calculate_baseline_stats()
        self.logger = logging.getLogger(__name__)
    
    def _calculate_baseline_stats(self) -> Dict[str, Any]:
        """计算基线统计信息"""
        stats = {}
        
        # 数值特征统计
        numeric_cols = self.baseline_data.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            stats[f'{col}_mean'] = self.baseline_data[col].mean()
            stats[f'{col}_std'] = self.baseline_data[col].std()
            stats[f'{col}_min'] = self.baseline_data[col].min()
            stats[f'{col}_max'] = self.baseline_data[col].max()
        
        # 分类特征统计
        categorical_cols = self.baseline_data.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            stats[f'{col}_distribution'] = self.baseline_data[col].value_counts(normalize=True).to_dict()
        
        # 预测分布
        stats['prediction_mean'] = np.mean(self.baseline_predictions)
        stats['prediction_std'] = np.std(self.baseline_predictions)
        
        return stats
    
    def detect_data_drift(self, current_data: pd.DataFrame, 
                         threshold: float = 0.05) -> Dict[str, Any]:
        """检测数据漂移"""
        drift_results = {}
        
        # 数值特征漂移检测
        numeric_cols = current_data.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            baseline_mean = self.baseline_stats[f'{col}_mean']
            baseline_std = self.baseline_stats[f'{col}_std']
            current_mean = current_data[col].mean()
            current_std = current_data[col].std()
            
            # KS检验
            baseline_sample = np.random.normal(baseline_mean, baseline_std, 1000)
            current_sample = current_data[col].values
            
            ks_statistic, p_value = stats.ks_2samp(baseline_sample, current_sample)
            
            drift_results[col] = {
                'ks_statistic': ks_statistic,
                'p_value': p_value,
                'drift_detected': p_value < threshold,
                'baseline_mean': baseline_mean,
                'current_mean': current_mean,
                'mean_shift': abs(current_mean - baseline_mean) / baseline_std
            }
        
        # 分类特征漂移检测
        categorical_cols = current_data.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            baseline_dist = self.baseline_stats[f'{col}_distribution']
            current_dist = current_data[col].value_counts(normalize=True).to_dict()
            
            # 计算分布差异
            all_categories = set(baseline_dist.keys()) | set(current_dist.keys())
            baseline_vec = np.array([baseline_dist.get(cat, 0) for cat in all_categories])
            current_vec = np.array([current_dist.get(cat, 0) for cat in all_categories])
            
            # JS散度
            js_divergence = self._calculate_js_divergence(baseline_vec, current_vec)
            
            drift_results[col] = {
                'js_divergence': js_divergence,
                'drift_detected': js_divergence > threshold,
                'baseline_distribution': baseline_dist,
                'current_distribution': current_dist
            }
        
        return drift_results
    
    def detect_prediction_drift(self, current_predictions: np.ndarray,
                               threshold: float = 0.05) -> Dict[str, Any]:
        """检测预测漂移"""
        baseline_mean = self.baseline_stats['prediction_mean']
        baseline_std = self.baseline_stats['prediction_std']
        current_mean = np.mean(current_predictions)
        current_std = np.std(current_predictions)
        
        # KS检验
        baseline_sample = np.random.normal(baseline_mean, baseline_std, 1000)
        ks_statistic, p_value = stats.ks_2samp(baseline_sample, current_predictions)
        
        return {
            'ks_statistic': ks_statistic,
            'p_value': p_value,
            'drift_detected': p_value < threshold,
            'baseline_mean': baseline_mean,
            'current_mean': current_mean,
            'mean_shift': abs(current_mean - baseline_mean) / baseline_std,
            'baseline_std': baseline_std,
            'current_std': current_std
        }
    
    def detect_performance_drift(self, current_predictions: np.ndarray,
                               current_labels: np.ndarray,
                               baseline_performance: float,
                               threshold: float = 0.05) -> Dict[str, Any]:
        """检测性能漂移"""
        current_performance = roc_auc_score(current_labels, current_predictions)
        performance_drop = baseline_performance - current_performance
        
        return {
            'baseline_performance': baseline_performance,
            'current_performance': current_performance,
            'performance_drop': performance_drop,
            'drift_detected': performance_drop > threshold,
            'relative_drop': performance_drop / baseline_performance
        }
    
    def _calculate_js_divergence(self, p: np.ndarray, q: np.ndarray) -> float:
        """计算JS散度"""
        # 归一化
        p = p / np.sum(p)
        q = q / np.sum(q)
        
        # 计算KL散度
        def kl_divergence(p, q):
            return np.sum(p * np.log(p / q + 1e-10))
        
        # JS散度
        m = 0.5 * (p + q)
        js_div = 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)
        
        return js_div
```

**实时监控系统**：
```python
import time
import threading
from typing import Dict, Any, List, Callable
import logging
from collections import deque
import json

class ModelMonitoringSystem:
    def __init__(self, model_name: str, check_interval: int = 60):
        self.model_name = model_name
        self.check_interval = check_interval
        self.monitoring_active = False
        self.monitoring_thread = None
        self.logger = logging.getLogger(__name__)
        
        # 监控数据存储
        self.metrics_history = deque(maxlen=1000)
        self.alerts = []
        
        # 监控指标
        self.metrics = {
            'prediction_count': 0,
            'error_count': 0,
            'avg_response_time': 0,
            'p95_response_time': 0,
            'p99_response_time': 0,
            'throughput': 0
        }
        
        # 阈值配置
        self.thresholds = {
            'error_rate': 0.05,
            'avg_response_time': 1.0,
            'p95_response_time': 2.0,
            'p99_response_time': 5.0,
            'throughput_drop': 0.5
        }
    
    def start_monitoring(self):
        """启动监控"""
        if not self.monitoring_active:
            self.monitoring_active = True
            self.monitoring_thread = threading.Thread(target=self._monitoring_loop)
            self.monitoring_thread.daemon = True
            self.monitoring_thread.start()
            self.logger.info(f"Started monitoring for model: {self.model_name}")
    
    def stop_monitoring(self):
        """停止监控"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join()
        self.logger.info(f"Stopped monitoring for model: {self.model_name}")
    
    def _monitoring_loop(self):
        """监控循环"""
        while self.monitoring_active:
            try:
                # 收集指标
                current_metrics = self._collect_metrics()
                
                # 检查阈值
                alerts = self._check_thresholds(current_metrics)
                
                # 记录指标历史
                self.metrics_history.append({
                    'timestamp': time.time(),
                    'metrics': current_metrics,
                    'alerts': alerts
                })
                
                # 处理告警
                if alerts:
                    self._handle_alerts(alerts)
                
                time.sleep(self.check_interval)
                
            except Exception as e:
                self.logger.error(f"Monitoring error: {e}")
                time.sleep(self.check_interval)
    
    def _collect_metrics(self) -> Dict[str, Any]:
        """收集指标"""
        # 这里应该从实际的模型服务中收集指标
        # 简化版本，实际应用中需要连接到监控系统
        
        return {
            'prediction_count': self.metrics['prediction_count'],
            'error_count': self.metrics['error_count'],
            'error_rate': self.metrics['error_count'] / max(self.metrics['prediction_count'], 1),
            'avg_response_time': self.metrics['avg_response_time'],
            'p95_response_time': self.metrics['p95_response_time'],
            'p99_response_time': self.metrics['p99_response_time'],
            'throughput': self.metrics['throughput']
        }
    
    def _check_thresholds(self, metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """检查阈值"""
        alerts = []
        
        # 错误率检查
        if metrics['error_rate'] > self.thresholds['error_rate']:
            alerts.append({
                'type': 'error_rate',
                'value': metrics['error_rate'],
                'threshold': self.thresholds['error_rate'],
                'severity': 'high',
                'message': f"Error rate {metrics['error_rate']:.2%} exceeds threshold {self.thresholds['error_rate']:.2%}"
            })
        
        # 响应时间检查
        if metrics['avg_response_time'] > self.thresholds['avg_response_time']:
            alerts.append({
                'type': 'avg_response_time',
                'value': metrics['avg_response_time'],
                'threshold': self.thresholds['avg_response_time'],
                'severity': 'medium',
                'message': f"Average response time {metrics['avg_response_time']:.2f}s exceeds threshold {self.thresholds['avg_response_time']:.2f}s"
            })
        
        if metrics['p95_response_time'] > self.thresholds['p95_response_time']:
            alerts.append({
                'type': 'p95_response_time',
                'value': metrics['p95_response_time'],
                'threshold': self.thresholds['p95_response_time'],
                'severity': 'high',
                'message': f"P95 response time {metrics['p95_response_time']:.2f}s exceeds threshold {self.thresholds['p95_response_time']:.2f}s"
            })
        
        return alerts
    
    def _handle_alerts(self, alerts: List[Dict[str, Any]]):
        """处理告警"""
        for alert in alerts:
            self.alerts.append({
                'timestamp': time.time(),
                'model_name': self.model_name,
                **alert
            })
            
            # 发送告警通知
            self._send_alert_notification(alert)
    
    def _send_alert_notification(self, alert: Dict[str, Any]):
        """发送告警通知"""
        # 这里应该集成实际的告警系统（如邮件、Slack、PagerDuty等）
        self.logger.warning(f"ALERT: {alert['message']}")
        
        # 示例：发送到日志
        alert_data = {
            'timestamp': time.time(),
            'model_name': self.model_name,
            'alert': alert
        }
        
        # 实际应用中应该发送到告警系统
        print(f"ALERT: {json.dumps(alert_data, indent=2)}")
    
    def update_metrics(self, prediction_time: float, error: bool = False):
        """更新指标"""
        self.metrics['prediction_count'] += 1
        if error:
            self.metrics['error_count'] += 1
        
        # 更新响应时间指标（简化版本）
        # 实际应用中需要维护响应时间分布
        self.metrics['avg_response_time'] = prediction_time
        self.metrics['p95_response_time'] = prediction_time * 1.5
        self.metrics['p99_response_time'] = prediction_time * 2.0
    
    def get_metrics_summary(self) -> Dict[str, Any]:
        """获取指标摘要"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = list(self.metrics_history)[-10:]  # 最近10个数据点
        
        return {
            'model_name': self.model_name,
            'current_metrics': self.metrics,
            'recent_trends': recent_metrics,
            'active_alerts': len([a for a in self.alerts if time.time() - a['timestamp'] < 3600]),  # 1小时内的告警
            'monitoring_status': 'active' if self.monitoring_active else 'inactive'
        }
    
    def get_alerts_history(self, hours: int = 24) -> List[Dict[str, Any]]:
        """获取告警历史"""
        cutoff_time = time.time() - (hours * 3600)
        return [alert for alert in self.alerts if alert['timestamp'] > cutoff_time]
```

## 八、特征平台设计

### 8.1 特征存储与管理

**特征存储系统**：
```python
import redis
import json
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Union
import logging
from datetime import datetime, timedelta
import pickle

class FeatureStore:
    def __init__(self, redis_client: redis.Redis, db_client=None):
        self.redis_client = redis_client
        self.db_client = db_client
        self.logger = logging.getLogger(__name__)
        
        # 特征元数据
        self.feature_metadata = {}
        self.load_feature_metadata()
    
    def load_feature_metadata(self):
        """加载特征元数据"""
        try:
            metadata_str = self.redis_client.get('feature_metadata')
            if metadata_str:
                self.feature_metadata = json.loads(metadata_str)
        except Exception as e:
            self.logger.error(f"Failed to load feature metadata: {e}")
            self.feature_metadata = {}
    
    def save_feature_metadata(self):
        """保存特征元数据"""
        try:
            self.redis_client.set('feature_metadata', json.dumps(self.feature_metadata))
        except Exception as e:
            self.logger.error(f"Failed to save feature metadata: {e}")
    
    def register_feature(self, feature_name: str, feature_config: Dict[str, Any]):
        """注册特征"""
        self.feature_metadata[feature_name] = {
            'name': feature_name,
            'type': feature_config['type'],
            'description': feature_config.get('description', ''),
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat(),
            'ttl': feature_config.get('ttl', 86400),  # 默认24小时
            'version': feature_config.get('version', '1.0'),
            'tags': feature_config.get('tags', []),
            'schema': feature_config.get('schema', {})
        }
        
        self.save_feature_metadata()
        self.logger.info(f"Registered feature: {feature_name}")
    
    def store_feature(self, feature_name: str, entity_id: str, 
                     feature_value: Any, timestamp: Optional[datetime] = None):
        """存储特征值"""
        if feature_name not in self.feature_metadata:
            raise ValueError(f"Feature {feature_name} not registered")
        
        if timestamp is None:
            timestamp = datetime.now()
        
        # 构建存储键
        feature_key = f"feature:{feature_name}:{entity_id}"
        
        # 序列化特征值
        if isinstance(feature_value, (pd.DataFrame, pd.Series)):
            feature_data = pickle.dumps(feature_value)
        elif isinstance(feature_value, np.ndarray):
            feature_data = pickle.dumps(feature_value)
        else:
            feature_data = json.dumps(feature_value)
        
        # 存储到Redis
        feature_info = {
            'value': feature_data.decode() if isinstance(feature_data, bytes) else feature_data,
            'timestamp': timestamp.isoformat(),
            'type': type(feature_value).__name__
        }
        
        ttl = self.feature_metadata[feature_name]['ttl']
        self.redis_client.setex(
            feature_key,
            ttl,
            json.dumps(feature_info)
        )
        
        # 更新特征元数据
        self.feature_metadata[feature_name]['updated_at'] = datetime.now().isoformat()
        self.save_feature_metadata()
    
    def get_feature(self, feature_name: str, entity_id: str) -> Optional[Any]:
        """获取特征值"""
        feature_key = f"feature:{feature_name}:{entity_id}"
        
        try:
            feature_data = self.redis_client.get(feature_key)
            if not feature_data:
                return None
            
            feature_info = json.loads(feature_data)
            
            # 反序列化特征值
            if feature_info['type'] in ['DataFrame', 'Series', 'ndarray']:
                return pickle.loads(feature_info['value'].encode())
            else:
                return json.loads(feature_info['value'])
        
        except Exception as e:
            self.logger.error(f"Failed to get feature {feature_name} for entity {entity_id}: {e}")
            return None
    
    def get_features(self, feature_names: List[str], entity_id: str) -> Dict[str, Any]:
        """批量获取特征"""
        features = {}
        
        for feature_name in feature_names:
            feature_value = self.get_feature(feature_name, entity_id)
            if feature_value is not None:
                features[feature_name] = feature_value
        
        return features
    
    def get_feature_history(self, feature_name: str, entity_id: str, 
                          hours: int = 24) -> List[Dict[str, Any]]:
        """获取特征历史"""
        # 这里简化实现，实际应用中需要更复杂的历史数据管理
        feature_key = f"feature:{feature_name}:{entity_id}"
        
        try:
            feature_data = self.redis_client.get(feature_key)
            if not feature_data:
                return []
            
            feature_info = json.loads(feature_data)
            timestamp = datetime.fromisoformat(feature_info['timestamp'])
            
            if timestamp > datetime.now() - timedelta(hours=hours):
                return [feature_info]
            else:
                return []
        
        except Exception as e:
            self.logger.error(f"Failed to get feature history: {e}")
            return []
    
    def batch_store_features(self, features_data: Dict[str, Dict[str, Any]]):
        """批量存储特征"""
        pipeline = self.redis_client.pipeline()
        
        for feature_name, entities_data in features_data.items():
            if feature_name not in self.feature_metadata:
                self.logger.warning(f"Feature {feature_name} not registered, skipping")
                continue
            
            ttl = self.feature_metadata[feature_name]['ttl']
            
            for entity_id, feature_value in entities_data.items():
                feature_key = f"feature:{feature_name}:{entity_id}"
                
                # 序列化特征值
                if isinstance(feature_value, (pd.DataFrame, pd.Series)):
                    feature_data = pickle.dumps(feature_value)
                elif isinstance(feature_value, np.ndarray):
                    feature_data = pickle.dumps(feature_value)
                else:
                    feature_data = json.dumps(feature_value)
                
                feature_info = {
                    'value': feature_data.decode() if isinstance(feature_data, bytes) else feature_data,
                    'timestamp': datetime.now().isoformat(),
                    'type': type(feature_value).__name__
                }
                
                pipeline.setex(feature_key, ttl, json.dumps(feature_info))
        
        pipeline.execute()
        self.logger.info(f"Batch stored features for {len(features_data)} feature types")
    
    def list_features(self) -> List[Dict[str, Any]]:
        """列出所有特征"""
        return list(self.feature_metadata.values())
    
    def get_feature_stats(self, feature_name: str) -> Dict[str, Any]:
        """获取特征统计信息"""
        if feature_name not in self.feature_metadata:
            return {}
        
        # 获取所有该特征的键
        pattern = f"feature:{feature_name}:*"
        keys = self.redis_client.keys(pattern)
        
        stats = {
            'feature_name': feature_name,
            'total_entities': len(keys),
            'metadata': self.feature_metadata[feature_name]
        }
        
        # 计算特征值统计（简化版本）
        if keys:
            sample_values = []
            for key in keys[:100]:  # 采样前100个
                try:
                    feature_data = self.redis_client.get(key)
                    if feature_data:
                        feature_info = json.loads(feature_data)
                        value = json.loads(feature_info['value'])
                        if isinstance(value, (int, float)):
                            sample_values.append(value)
                except:
                    continue
            
            if sample_values:
                stats['sample_stats'] = {
                    'count': len(sample_values),
                    'mean': np.mean(sample_values),
                    'std': np.std(sample_values),
                    'min': np.min(sample_values),
                    'max': np.max(sample_values)
                }
        
        return stats
```

### 8.2 特征工程流水线

**特征工程流水线**：
```python
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Callable, Optional
import logging
from datetime import datetime, timedelta
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.feature_selection import SelectKBest, f_classif
import joblib

class FeatureEngineeringPipeline:
    def __init__(self, feature_store: FeatureStore):
        self.feature_store = feature_store
        self.transformers = {}
        self.feature_selectors = {}
        self.logger = logging.getLogger(__name__)
        
        # 特征工程步骤
        self.steps = []
    
    def add_step(self, step_name: str, transformer: Callable, 
                input_features: List[str], output_features: List[str]):
        """添加特征工程步骤"""
        step = {
            'name': step_name,
            'transformer': transformer,
            'input_features': input_features,
            'output_features': output_features,
            'fitted': False
        }
        
        self.steps.append(step)
        self.logger.info(f"Added feature engineering step: {step_name}")
    
    def fit_transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """拟合并转换数据"""
        transformed_data = data.copy()
        
        for step in self.steps:
            try:
                # 提取输入特征
                input_data = transformed_data[step['input_features']]
                
                # 拟合转换器
                if not step['fitted']:
                    step['transformer'].fit(input_data)
                    step['fitted'] = True
                
                # 转换数据
                transformed_features = step['transformer'].transform(input_data)
                
                # 添加输出特征
                if isinstance(transformed_features, np.ndarray):
                    if len(transformed_features.shape) == 1:
                        transformed_data[step['output_features'][0]] = transformed_features
                    else:
                        for i, output_feature in enumerate(step['output_features']):
                            transformed_data[output_feature] = transformed_features[:, i]
                else:
                    transformed_data[step['output_features'][0]] = transformed_features
                
                self.logger.info(f"Applied step: {step['name']}")
                
            except Exception as e:
                self.logger.error(f"Error in step {step['name']}: {e}")
                raise
        
        return transformed_data
    
    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """转换数据（不拟合）"""
        transformed_data = data.copy()
        
        for step in self.steps:
            if not step['fitted']:
                raise ValueError(f"Step {step['name']} not fitted yet")
            
            try:
                # 提取输入特征
                input_data = transformed_data[step['input_features']]
                
                # 转换数据
                transformed_features = step['transformer'].transform(input_data)
                
                # 添加输出特征
                if isinstance(transformed_features, np.ndarray):
                    if len(transformed_features.shape) == 1:
                        transformed_data[step['output_features'][0]] = transformed_features
                    else:
                        for i, output_feature in enumerate(step['output_features']):
                            transformed_data[output_feature] = transformed_features[:, i]
                else:
                    transformed_data[step['output_features'][0]] = transformed_features
                
            except Exception as e:
                self.logger.error(f"Error in step {step['name']}: {e}")
                raise
        
        return transformed_data
    
    def save_pipeline(self, filepath: str):
        """保存流水线"""
        pipeline_data = {
            'steps': self.steps,
            'transformers': self.transformers,
            'feature_selectors': self.feature_selectors
        }
        
        joblib.dump(pipeline_data, filepath)
        self.logger.info(f"Saved pipeline to {filepath}")
    
    def load_pipeline(self, filepath: str):
        """加载流水线"""
        pipeline_data = joblib.load(filepath)
        
        self.steps = pipeline_data['steps']
        self.transformers = pipeline_data['transformers']
        self.feature_selectors = pipeline_data['feature_selectors']
        
        self.logger.info(f"Loaded pipeline from {filepath}")
    
    def create_standard_pipeline(self, numeric_features: List[str], 
                               categorical_features: List[str],
                               target_feature: Optional[str] = None):
        """创建标准特征工程流水线"""
        
        # 数值特征标准化
        if numeric_features:
            scaler = StandardScaler()
            self.add_step(
                'numeric_scaling',
                scaler,
                numeric_features,
                [f"{feat}_scaled" for feat in numeric_features]
            )
        
        # 分类特征编码
        if categorical_features:
            # 标签编码
            for cat_feat in categorical_features:
                encoder = LabelEncoder()
                self.add_step(
                    f'{cat_feat}_label_encoding',
                    encoder,
                    [cat_feat],
                    [f"{cat_feat}_encoded"]
                )
        
        # 特征选择（如果有目标特征）
        if target_feature:
            selector = SelectKBest(score_func=f_classif, k=10)
            all_features = numeric_features + [f"{feat}_encoded" for feat in categorical_features]
            self.add_step(
                'feature_selection',
                selector,
                all_features,
                [f"selected_feature_{i}" for i in range(10)]
            )
    
    def generate_feature_importance(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """生成特征重要性"""
        from sklearn.ensemble import RandomForestClassifier
        
        # 使用随机森林计算特征重要性
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(X, y)
        
        feature_importance = dict(zip(X.columns, rf.feature_importances_))
        
        # 按重要性排序
        sorted_importance = dict(sorted(feature_importance.items(), 
                                      key=lambda x: x[1], reverse=True))
        
        return sorted_importance
```

## 九、在线学习系统

### 9.1 增量学习

**在线学习系统**：
```python
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
import logging
from sklearn.linear_model import SGDClassifier, SGDRegressor
from sklearn.preprocessing import StandardScaler
import joblib
import threading
import time
from collections import deque

class OnlineLearningSystem:
    def __init__(self, model_type: str = 'classification', 
                 learning_rate: float = 0.01,
                 batch_size: int = 1000):
        self.model_type = model_type
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.logger = logging.getLogger(__name__)
        
        # 初始化模型
        if model_type == 'classification':
            self.model = SGDClassifier(
                learning_rate='adaptive',
                eta0=learning_rate,
                random_state=42
            )
        else:
            self.model = SGDRegressor(
                learning_rate='adaptive',
                eta0=learning_rate,
                random_state=42
            )
        
        # 特征缩放器
        self.scaler = StandardScaler()
        self.is_fitted = False
        
        # 在线学习队列
        self.learning_queue = deque(maxlen=batch_size * 10)
        self.learning_thread = None
        self.learning_active = False
        
        # 性能监控
        self.performance_history = deque(maxlen=1000)
        self.model_versions = []
    
    def start_online_learning(self):
        """启动在线学习"""
        if not self.learning_active:
            self.learning_active = True
            self.learning_thread = threading.Thread(target=self._learning_loop)
            self.learning_thread.daemon = True
            self.learning_thread.start()
            self.logger.info("Started online learning")
    
    def stop_online_learning(self):
        """停止在线学习"""
        self.learning_active = False
        if self.learning_thread:
            self.learning_thread.join()
        self.logger.info("Stopped online learning")
    
    def _learning_loop(self):
        """在线学习循环"""
        while self.learning_active:
            try:
                if len(self.learning_queue) >= self.batch_size:
                    # 获取批次数据
                    batch_data = []
                    for _ in range(self.batch_size):
                        if self.learning_queue:
                            batch_data.append(self.learning_queue.popleft())
                    
                    if batch_data:
                        self._process_batch(batch_data)
                
                time.sleep(1)  # 每秒检查一次
                
            except Exception as e:
                self.logger.error(f"Error in learning loop: {e}")
                time.sleep(5)
    
    def _process_batch(self, batch_data: List[Dict[str, Any]]):
        """处理批次数据"""
        try:
            # 提取特征和标签
            X_batch = np.array([item['features'] for item in batch_data])
            y_batch = np.array([item['label'] for item in batch_data])
            
            # 特征缩放
            if not self.is_fitted:
                X_batch_scaled = self.scaler.fit_transform(X_batch)
                self.is_fitted = True
            else:
                X_batch_scaled = self.scaler.transform(X_batch)
            
            # 增量学习
            self.model.partial_fit(X_batch_scaled, y_batch)
            
            # 记录性能
            if self.model_type == 'classification':
                accuracy = self.model.score(X_batch_scaled, y_batch)
                self.performance_history.append({
                    'timestamp': time.time(),
                    'accuracy': accuracy,
                    'batch_size': len(batch_data)
                })
            else:
                mse = np.mean((self.model.predict(X_batch_scaled) - y_batch) ** 2)
                self.performance_history.append({
                    'timestamp': time.time(),
                    'mse': mse,
                    'batch_size': len(batch_data)
                })
            
            self.logger.info(f"Processed batch of {len(batch_data)} samples")
            
        except Exception as e:
            self.logger.error(f"Error processing batch: {e}")
    
    def add_training_data(self, features: np.ndarray, label: Any, 
                         metadata: Optional[Dict[str, Any]] = None):
        """添加训练数据"""
        data_point = {
            'features': features,
            'label': label,
            'metadata': metadata or {},
            'timestamp': time.time()
        }
        
        self.learning_queue.append(data_point)
    
    def predict(self, features: np.ndarray) -> Any:
        """预测"""
        if not self.is_fitted:
            raise ValueError("Model not fitted yet")
        
        features_scaled = self.scaler.transform(features.reshape(1, -1))
        return self.model.predict(features_scaled)[0]
    
    def predict_proba(self, features: np.ndarray) -> np.ndarray:
        """预测概率（仅分类）"""
        if self.model_type != 'classification':
            raise ValueError("Probability prediction only available for classification")
        
        if not self.is_fitted:
            raise ValueError("Model not fitted yet")
        
        features_scaled = self.scaler.transform(features.reshape(1, -1))
        return self.model.predict_proba(features_scaled)[0]
    
    def get_model_performance(self) -> Dict[str, Any]:
        """获取模型性能"""
        if not self.performance_history:
            return {}
        
        recent_performance = list(self.performance_history)[-10:]  # 最近10个批次
        
        if self.model_type == 'classification':
            avg_accuracy = np.mean([p['accuracy'] for p in recent_performance])
            return {
                'avg_accuracy': avg_accuracy,
                'recent_performance': recent_performance,
                'total_samples': len(self.performance_history)
            }
        else:
            avg_mse = np.mean([p['mse'] for p in recent_performance])
            return {
                'avg_mse': avg_mse,
                'recent_performance': recent_performance,
                'total_samples': len(self.performance_history)
            }
    
    def save_model(self, filepath: str):
        """保存模型"""
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'is_fitted': self.is_fitted,
            'model_type': self.model_type,
            'learning_rate': self.learning_rate,
            'batch_size': self.batch_size,
            'performance_history': list(self.performance_history)
        }
        
        joblib.dump(model_data, filepath)
        self.logger.info(f"Saved model to {filepath}")
    
    def load_model(self, filepath: str):
        """加载模型"""
        model_data = joblib.load(filepath)
        
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.is_fitted = model_data['is_fitted']
        self.model_type = model_data['model_type']
        self.learning_rate = model_data['learning_rate']
        self.batch_size = model_data['batch_size']
        self.performance_history = deque(model_data['performance_history'], maxlen=1000)
        
        self.logger.info(f"Loaded model from {filepath}")
    
    def create_model_version(self, version_name: str):
        """创建模型版本"""
        version_data = {
            'name': version_name,
            'timestamp': time.time(),
            'model': self.model,
            'scaler': self.scaler,
            'performance': self.get_model_performance()
        }
        
        self.model_versions.append(version_data)
        self.logger.info(f"Created model version: {version_name}")
    
    def rollback_to_version(self, version_name: str):
        """回滚到指定版本"""
        for version in reversed(self.model_versions):
            if version['name'] == version_name:
                self.model = version['model']
                self.scaler = version['scaler']
                self.logger.info(f"Rolled back to version: {version_name}")
                return
        
        raise ValueError(f"Version {version_name} not found")
```

## 十、高频面试题深度解析

### Q1：MLOps的核心组件有哪些？

**MLOps核心组件**：
1. **数据管理**：数据版本控制、数据质量监控、数据血缘追踪
2. **模型训练**：实验跟踪、超参数优化、分布式训练
3. **模型部署**：模型服务化、A/B测试、灰度发布
4. **模型监控**：性能监控、数据漂移检测、模型漂移检测
5. **特征工程**：特征存储、特征计算、特征服务
6. **在线学习**：增量学习、模型更新、版本管理

**代码示例**：
```python
class MLOpsPlatform:
    def __init__(self):
        self.data_manager = DataVersionControl()
        self.experiment_tracker = ExperimentTracker()
        self.model_registry = ModelRegistry()
        self.model_serving = ModelServingGateway()
        self.monitoring_system = ModelMonitoringSystem()
        self.feature_store = FeatureStore()
        self.online_learning = OnlineLearningSystem()
    
    def end_to_end_mlops_pipeline(self, data_path: str, model_config: Dict):
        """端到端MLOps流水线"""
        # 1. 数据管理
        data_version = self.data_manager.track_data(data_path, model_config['data_metadata'])
        
        # 2. 实验跟踪
        with self.experiment_tracker.start_run() as run:
            # 3. 模型训练
            model = self.train_model(data_path, model_config)
            
            # 4. 模型评估
            metrics = self.evaluate_model(model, model_config['test_data'])
            
            # 5. 模型注册
            model_version = self.model_registry.register_model(model, model_config['model_name'])
            
            # 6. 模型部署
            self.model_serving.deploy_model(model_version)
            
            # 7. 模型监控
            self.monitoring_system.start_monitoring(model_config['model_name'])
        
        return model_version
```

### Q2：如何处理模型的数据漂移问题？

**数据漂移检测与处理**：
1. **统计方法**：KS检验、PSI（Population Stability Index）、JS散度
2. **机器学习方法**：训练漂移检测模型
3. **实时监控**：设置阈值和告警
4. **自动处理**：模型重训练、特征工程调整

**代码示例**：
```python
class DataDriftHandler:
    def __init__(self, baseline_data: pd.DataFrame):
        self.baseline_data = baseline_data
        self.drift_detector = ModelDriftDetector(baseline_data, None)
        self.retraining_threshold = 0.1
        self.alert_threshold = 0.05
    
    def handle_drift(self, current_data: pd.DataFrame, 
                    model: Any, retrain_data: pd.DataFrame) -> Dict[str, Any]:
        """处理数据漂移"""
        # 检测漂移
        drift_results = self.drift_detector.detect_data_drift(current_data)
        
        # 计算漂移严重程度
        max_drift = max([result.get('ks_statistic', 0) for result in drift_results.values()])
        
        response = {
            'drift_detected': max_drift > self.alert_threshold,
            'drift_severity': max_drift,
            'action_taken': 'none'
        }
        
        if max_drift > self.retraining_threshold:
            # 严重漂移，需要重训练
            new_model = self.retrain_model(model, retrain_data)
            response['action_taken'] = 'retrain'
            response['new_model'] = new_model
        
        elif max_drift > self.alert_threshold:
            # 轻微漂移，发送告警
            self.send_drift_alert(drift_results)
            response['action_taken'] = 'alert'
        
        return response
    
    def retrain_model(self, model: Any, retrain_data: pd.DataFrame) -> Any:
        """重训练模型"""
        # 实现模型重训练逻辑
        # 这里简化处理
        return model
    
    def send_drift_alert(self, drift_results: Dict[str, Any]):
        """发送漂移告警"""
        # 实现告警发送逻辑
        pass
```

### Q3：模型服务化的最佳实践是什么？

**模型服务化最佳实践**：
1. **容器化部署**：使用Docker容器化模型服务
2. **负载均衡**：多实例部署，负载均衡
3. **健康检查**：定期健康检查和自动恢复
4. **版本管理**：支持多版本并存和灰度发布
5. **监控告警**：性能监控和异常告警
6. **安全防护**：API限流、认证授权

**代码示例**：
```python
class ProductionModelService:
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model_versions = {}
        self.current_version = None
        self.health_check_interval = 30
        self.max_concurrent_requests = 1000
        
    def deploy_model(self, model_version: str, model: Any):
        """部署模型"""
        # 加载模型
        self.model_versions[model_version] = model
        
        # 健康检查
        if self.health_check(model):
            self.current_version = model_version
            self.start_health_monitoring()
            return True
        else:
            return False
    
    def health_check(self, model: Any) -> bool:
        """健康检查"""
        try:
            # 使用测试数据检查模型
            test_data = np.random.randn(1, 10)  # 假设10个特征
            prediction = model.predict(test_data)
            return True
        except:
            return False
    
    def predict(self, data: np.ndarray) -> Any:
        """预测接口"""
        if not self.current_version:
            raise ValueError("No model deployed")
        
        model = self.model_versions[self.current_version]
        return model.predict(data)
    
    def canary_deployment(self, new_version: str, traffic_percentage: float = 10):
        """金丝雀部署"""
        # 实现金丝雀部署逻辑
        # 逐步增加新版本流量
        pass
```

### Q4：如何设计特征平台？

**特征平台设计**：
1. **特征存储**：支持离线特征和在线特征
2. **特征计算**：批处理和流处理特征计算
3. **特征服务**：低延迟特征查询服务
4. **特征管理**：特征元数据管理和版本控制
5. **特征监控**：特征质量监控和告警

**代码示例**：
```python
class FeaturePlatform:
    def __init__(self):
        self.feature_store = FeatureStore()
        self.feature_compute_engine = FeatureComputeEngine()
        self.feature_service = FeatureService()
        self.feature_monitor = FeatureMonitor()
    
    def design_feature_platform(self):
        """设计特征平台架构"""
        architecture = {
            'offline_features': {
                'storage': 'HDFS/S3',
                'compute': 'Spark/Flink',
                'format': 'Parquet/Delta'
            },
            'online_features': {
                'storage': 'Redis/DynamoDB',
                'compute': 'Flink/Kafka Streams',
                'format': 'JSON/Protobuf'
            },
            'feature_service': {
                'api': 'REST/GraphQL',
                'caching': 'Redis',
                'load_balancing': 'Nginx'
            },
            'feature_management': {
                'metadata': 'MySQL/PostgreSQL',
                'version_control': 'Git',
                'lineage': 'Apache Atlas'
            }
        }
        
        return architecture
```

### Q5：在线学习系统如何保证稳定性？

**在线学习稳定性保证**：
1. **数据质量**：输入数据验证和清洗
2. **模型验证**：在线A/B测试和性能监控
3. **回滚机制**：模型版本管理和快速回滚
4. **异常处理**：异常检测和自动恢复
5. **性能监控**：实时性能监控和告警

**代码示例**：
```python
class StableOnlineLearning:
    def __init__(self):
        self.online_learning = OnlineLearningSystem()
        self.model_validator = ModelValidator()
        self.performance_monitor = PerformanceMonitor()
        self.rollback_manager = RollbackManager()
    
    def stable_online_learning(self, new_data: Dict[str, Any]) -> Dict[str, Any]:
        """稳定的在线学习"""
        try:
            # 1. 数据验证
            if not self.validate_input_data(new_data):
                return {'status': 'rejected', 'reason': 'invalid_data'}
            
            # 2. 添加训练数据
            self.online_learning.add_training_data(
                new_data['features'], 
                new_data['label']
            )
            
            # 3. 性能监控
            performance = self.online_learning.get_model_performance()
            
            # 4. 性能验证
            if not self.model_validator.validate_performance(performance):
                # 性能下降，触发回滚
                self.rollback_manager.rollback_to_previous_version()
                return {'status': 'rolled_back', 'reason': 'performance_degradation'}
            
            # 5. 创建新版本
            version_name = f"v{int(time.time())}"
            self.online_learning.create_model_version(version_name)
            
            return {
                'status': 'success',
                'version': version_name,
                'performance': performance
            }
            
        except Exception as e:
            # 异常处理
            self.rollback_manager.rollback_to_previous_version()
            return {'status': 'error', 'reason': str(e)}
    
    def validate_input_data(self, data: Dict[str, Any]) -> bool:
        """验证输入数据"""
        # 实现数据验证逻辑
        return True
```

---

**关键字**：AI工程化、MLOps、模型服务化、特征工程、在线学习、模型监控、数据漂移、A/B测试、模型部署、特征平台

