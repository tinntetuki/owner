# 大数据技术栈深度解析

## 目录
- [一、Hadoop生态](#一hadoop生态)
- [二、Spark](#二spark)
- [三、Flink](#三flink)
- [四、数据仓库](#四数据仓库)
- [五、高频面试题](#五高频面试题)

## 一、Hadoop生态

### 1.1 HDFS

**架构**：
- **NameNode**：元数据管理
- **DataNode**：数据存储
- **Block**：128MB

**读写流程**：
```
写入流程：
Client → NameNode（获取DataNode） → DataNode1 → DataNode2 → DataNode3

读取流程：
Client → NameNode（获取Block位置） → 就近DataNode读取
```

### 1.2 MapReduce

**流程**：
```
Input → Map → Shuffle → Reduce → Output
```

**WordCount示例**：
```java
public class WordCount {
    
    public static class TokenizerMapper 
            extends Mapper<Object, Text, Text, IntWritable> {
        
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();
        
        public void map(Object key, Text value, Context context) 
                throws IOException, InterruptedException {
            StringTokenizer itr = new StringTokenizer(value.toString());
            while (itr.hasMoreTokens()) {
                word.set(itr.nextToken());
                context.write(word, one);
            }
        }
    }
    
    public static class IntSumReducer 
            extends Reducer<Text, IntWritable, Text, IntWritable> {
        
        public void reduce(Text key, Iterable<IntWritable> values, Context context) 
                throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            context.write(key, new IntWritable(sum));
        }
    }
}
```

### 1.3 YARN

**组件**：
- **ResourceManager**：资源管理
- **NodeManager**：节点管理
- **ApplicationMaster**：应用管理
- **Container**：资源容器

## 二、Spark

### 2.1 核心概念

- **RDD**：弹性分布式数据集
- **DataFrame**：结构化数据
- **Dataset**：类型安全的DataFrame

### 2.2 RDD操作

```scala
val sc = new SparkContext(conf)

// Transformation（转换操作，惰性执行）
val rdd1 = sc.textFile("hdfs://path/to/file")
val rdd2 = rdd1.flatMap(_.split(" "))
val rdd3 = rdd2.map((_, 1))
val rdd4 = rdd3.reduceByKey(_ + _)

// Action（行动操作，触发执行）
val result = rdd4.collect()
rdd4.saveAsTextFile("hdfs://path/to/output")
```

### 2.3 Spark SQL

```scala
val spark = SparkSession.builder().appName("Example").getOrCreate()

// DataFrame
val df = spark.read.json("people.json")
df.select("name", "age").filter($"age" > 21).show()

// SQL
df.createOrReplaceTempView("people")
spark.sql("SELECT name, age FROM people WHERE age > 21").show()
```

### 2.4 核心特性

1. **内存计算**：数据缓存在内存
2. **DAG执行引擎**：优化执行计划
3. **丰富的API**：Scala、Java、Python
4. **生态完整**：Spark SQL、Streaming、MLlib、GraphX

## 三、Flink

### 3.1 核心概念

- **DataStream**：数据流
- **Window**：窗口
- **Watermark**：水位线
- **State**：状态

### 3.2 DataStream API

```java
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

DataStream<String> text = env.socketTextStream("localhost", 9999);

DataStream<Tuple2<String, Integer>> counts = text
    .flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>() {
        @Override
        public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {
            for (String word : value.split(" ")) {
                out.collect(new Tuple2<>(word, 1));
            }
        }
    })
    .keyBy(0)
    .sum(1);

counts.print();
env.execute("Streaming WordCount");
```

### 3.3 Window

```java
// 滚动窗口（Tumbling Window）
stream.keyBy(0)
    .window(TumblingProcessingTimeWindows.of(Time.seconds(10)))
    .sum(1);

// 滑动窗口（Sliding Window）
stream.keyBy(0)
    .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5)))
    .sum(1);

// 会话窗口（Session Window）
stream.keyBy(0)
    .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10)))
    .sum(1);
```

## 四、数据仓库

### 4.1 分层架构

```
ODS（Operational Data Store）：原始数据层
 ↓
DWD（Data Warehouse Detail）：明细数据层
 ↓
DWS（Data Warehouse Summary）：汇总数据层
 ↓
ADS（Application Data Store）：应用数据层
```

### 4.2 Hive

**建表**：
```sql
CREATE TABLE user_behavior (
    user_id BIGINT,
    item_id BIGINT,
    category_id BIGINT,
    behavior STRING,
    timestamp BIGINT
)
PARTITIONED BY (dt STRING)
STORED AS PARQUET;
```

**查询**：
```sql
SELECT 
    category_id,
    COUNT(DISTINCT user_id) AS uv,
    COUNT(*) AS pv
FROM user_behavior
WHERE dt = '2024-01-01'
AND behavior = 'click'
GROUP BY category_id;
```

## 五、高频面试题

### Q1：Spark和MapReduce的区别？

| 特性 | Spark | MapReduce |
|------|-------|-----------|
| 速度 | 快（内存计算） | 慢（磁盘IO） |
| 易用性 | 高（丰富API） | 低（冗长代码） |
| 实时性 | 支持 | 不支持 |

### Q2：Flink和Spark Streaming的区别？

| 特性 | Flink | Spark Streaming |
|------|-------|-----------------|
| 处理模式 | 真流处理 | 微批处理 |
| 延迟 | ms级 | 秒级 |
| 状态管理 | 更完善 | 基础 |

### Q3：数据倾斜如何解决？

1. **预聚合**：增加Combiner
2. **加盐**：给key加随机前缀
3. **采样**：大key单独处理
4. **广播小表**：Map端Join

---

**关键字**：Hadoop、HDFS、Spark、Flink、Hive、数据仓库

