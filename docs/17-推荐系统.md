# 推荐系统深度解析

## 目录
- [一、协同过滤](#一协同过滤)
- [二、深度学习推荐](#二深度学习推荐)
- [三、实时推荐架构](#三实时推荐架构)
- [四、高频面试题](#四高频面试题)

## 一、协同过滤

### 1.1 UserCF（基于用户）

```python
import numpy as np
from scipy.spatial.distance import cosine

class UserCF:
    def __init__(self, ratings_matrix):
        self.ratings_matrix = ratings_matrix
        self.similarity_matrix = self._calculate_similarity()
    
    def _calculate_similarity(self):
        n_users = self.ratings_matrix.shape[0]
        similarity = np.zeros((n_users, n_users))
        
        for i in range(n_users):
            for j in range(i+1, n_users):
                sim = 1 - cosine(
                    self.ratings_matrix[i],
                    self.ratings_matrix[j]
                )
                similarity[i, j] = similarity[j, i] = sim
        
        return similarity
    
    def recommend(self, user_id, k=10):
        # 找到最相似的用户
        similar_users = np.argsort(self.similarity_matrix[user_id])[::-1][1:k+1]
        
        # 推荐物品
        recommendations = {}
        for user in similar_users:
            for item_id, rating in enumerate(self.ratings_matrix[user]):
                if rating > 0 and self.ratings_matrix[user_id][item_id] == 0:
                    if item_id not in recommendations:
                        recommendations[item_id] = 0
                    recommendations[item_id] += rating * self.similarity_matrix[user_id][user]
        
        return sorted(recommendations.items(), key=lambda x: x[1], reverse=True)[:k]
```

### 1.2 ItemCF（基于物品）

```python
class ItemCF:
    def __init__(self, ratings_matrix):
        self.ratings_matrix = ratings_matrix.T  # 转置
        self.similarity_matrix = self._calculate_similarity()
    
    def recommend(self, user_id, k=10):
        user_ratings = self.ratings_matrix[:, user_id]
        scores = {}
        
        for item_id in range(len(user_ratings)):
            if user_ratings[item_id] > 0:
                continue
            
            score = 0
            for rated_item in np.where(user_ratings > 0)[0]:
                score += (self.similarity_matrix[item_id][rated_item] * 
                         user_ratings[rated_item])
            
            scores[item_id] = score
        
        return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]
```

### 1.3 矩阵分解（MF）

```python
import numpy as np

class MatrixFactorization:
    def __init__(self, R, K, alpha=0.001, beta=0.02, iterations=100):
        self.R = R
        self.num_users, self.num_items = R.shape
        self.K = K
        self.alpha = alpha
        self.beta = beta
        self.iterations = iterations
    
    def train(self):
        # 初始化用户和物品矩阵
        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))
        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))
        
        for iteration in range(self.iterations):
            for i in range(self.num_users):
                for j in range(self.num_items):
                    if self.R[i, j] > 0:
                        error = self.R[i, j] - np.dot(self.P[i, :], self.Q[j, :].T)
                        
                        # 更新参数
                        self.P[i, :] += self.alpha * (error * self.Q[j, :] - self.beta * self.P[i, :])
                        self.Q[j, :] += self.alpha * (error * self.P[i, :] - self.beta * self.Q[j, :])
            
            # 计算损失
            loss = 0
            for i in range(self.num_users):
                for j in range(self.num_items):
                    if self.R[i, j] > 0:
                        loss += pow(self.R[i, j] - np.dot(self.P[i, :], self.Q[j, :].T), 2)
                        loss += (self.beta/2) * (pow(np.linalg.norm(self.P[i, :]), 2) + 
                                                 pow(np.linalg.norm(self.Q[j, :]), 2))
            
            if (iteration+1) % 10 == 0:
                print(f"Iteration: {iteration+1}, Loss: {loss}")
        
        return self.P, self.Q
    
    def predict(self, user_id, item_id):
        return np.dot(self.P[user_id, :], self.Q[item_id, :].T)
```

## 二、深度学习推荐

### 2.1 DeepFM

```python
import tensorflow as tf

class DeepFM(tf.keras.Model):
    def __init__(self, feature_columns, hidden_units):
        super(DeepFM, self).__init__()
        
        # FM部分
        self.fm_first_order = tf.keras.layers.DenseFeatures(feature_columns)
        self.fm_second_order = tf.keras.layers.DenseFeatures(feature_columns)
        
        # Deep部分
        self.dnn_layers = [
            tf.keras.layers.Dense(units, activation='relu') 
            for units in hidden_units
        ]
        self.dnn_output = tf.keras.layers.Dense(1)
        
        # 输出层
        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')
    
    def call(self, inputs):
        # FM一阶
        fm_first = self.fm_first_order(inputs)
        
        # FM二阶
        fm_second = self.fm_second_order(inputs)
        
        # Deep
        dnn = inputs
        for layer in self.dnn_layers:
            dnn = layer(dnn)
        dnn_output = self.dnn_output(dnn)
        
        # 组合
        output = self.output_layer(fm_first + fm_second + dnn_output)
        return output
```

### 2.2 Wide & Deep

```python
class WideAndDeep(tf.keras.Model):
    def __init__(self, wide_columns, deep_columns, deep_hidden_units):
        super(WideAndDeep, self).__init__()
        
        # Wide部分
        self.wide = tf.keras.layers.DenseFeatures(wide_columns)
        
        # Deep部分
        self.deep = tf.keras.layers.DenseFeatures(deep_columns)
        self.deep_layers = [
            tf.keras.layers.Dense(units, activation='relu')
            for units in deep_hidden_units
        ]
        
        # 输出层
        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')
    
    def call(self, inputs):
        wide_output = self.wide(inputs)
        
        deep_output = self.deep(inputs)
        for layer in self.deep_layers:
            deep_output = layer(deep_output)
        
        combined = tf.concat([wide_output, deep_output], axis=1)
        return self.output_layer(combined)
```

## 三、实时推荐架构

### 3.1 整体架构

```
用户行为 → Kafka → Flink(实时特征) → Redis
                      ↓
                  特征服务
                      ↓
模型训练 ← 离线特征 ← Spark
   ↓
模型服务 → 推荐结果 → 用户
```

### 3.2 特征工程

```python
class FeatureEngineer:
    def __init__(self):
        self.redis_client = redis.Redis()
    
    def get_user_features(self, user_id):
        # 实时特征（Redis）
        recent_clicks = self.redis_client.zrevrange(
            f'user:{user_id}:clicks', 0, 99
        )
        
        # 离线特征（数据库）
        user_profile = self.db.query(
            f"SELECT age, gender, city FROM users WHERE id = {user_id}"
        )
        
        return {
            'user_id': user_id,
            'recent_clicks': recent_clicks,
            'age': user_profile.age,
            'gender': user_profile.gender,
            'city': user_profile.city
        }
    
    def get_item_features(self, item_id):
        return {
            'item_id': item_id,
            'category': self.get_category(item_id),
            'price': self.get_price(item_id),
            'popularity': self.get_popularity(item_id)
        }
```

### 3.3 在线服务

```python
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/recommend', methods=['POST'])
def recommend():
    user_id = request.json['user_id']
    k = request.json.get('k', 10)
    
    # 1. 召回
    recall_items = recall_service.recall(user_id, k * 10)
    
    # 2. 排序
    features = feature_engineer.get_features(user_id, recall_items)
    scores = model.predict(features)
    
    # 3. 重排序
    final_items = rerank(recall_items, scores)[:k]
    
    return jsonify({
        'user_id': user_id,
        'items': final_items
    })
```

## 四、高频面试题

### Q1：UserCF和ItemCF的区别？

| 特性 | UserCF | ItemCF |
|------|--------|--------|
| 适用场景 | 用户少 | 物品少 |
| 实时性 | 差 | 好 |
| 解释性 | 较弱 | 较强 |

### Q2：冷启动问题如何解决？

1. **用户冷启动**：
   - 热门推荐
   - 基于人口统计学
   - 引导用户填写兴趣

2. **物品冷启动**：
   - 基于内容推荐
   - 专家推荐
   - 探索策略（ε-greedy）

### Q3：推荐系统如何评估？

**离线评估**：
- RMSE、MAE
- Precision、Recall、F1
- NDCG、MAP

**在线评估**：
- CTR（点击率）
- CVR（转化率）
- 停留时间

## 五、推荐算法完整体系

### 5.1 协同过滤算法深度实现

**基于用户的协同过滤（UserCF）优化版**：
```python
import numpy as np
from scipy.spatial.distance import cosine
from collections import defaultdict
import logging
from typing import List, Dict, Tuple, Optional
import heapq

class AdvancedUserCF:
    def __init__(self, ratings_matrix: np.ndarray, min_common_items: int = 5):
        self.ratings_matrix = ratings_matrix
        self.min_common_items = min_common_items
        self.user_similarity_cache = {}
        self.logger = logging.getLogger(__name__)
        
    def _calculate_user_similarity(self, user1: int, user2: int) -> float:
        """计算两个用户的相似度"""
        # 找到两个用户都评分的物品
        common_items = np.where(
            (self.ratings_matrix[user1] > 0) & 
            (self.ratings_matrix[user2] > 0)
        )[0]
        
        if len(common_items) < self.min_common_items:
            return 0.0
        
        # 计算皮尔逊相关系数
        ratings1 = self.ratings_matrix[user1][common_items]
        ratings2 = self.ratings_matrix[user2][common_items]
        
        # 中心化
        mean1, mean2 = np.mean(ratings1), np.mean(ratings2)
        centered1 = ratings1 - mean1
        centered2 = ratings2 - mean2
        
        # 计算相关系数
        numerator = np.dot(centered1, centered2)
        denominator = np.sqrt(np.sum(centered1**2) * np.sum(centered2**2))
        
        if denominator == 0:
            return 0.0
        
        correlation = numerator / denominator
        
        # 应用显著性权重
        significance_weight = min(len(common_items) / 50, 1.0)
        return correlation * significance_weight
    
    def get_similar_users(self, user_id: int, k: int = 20) -> List[Tuple[int, float]]:
        """获取最相似的k个用户"""
        if user_id in self.user_similarity_cache:
            return self.user_similarity_cache[user_id][:k]
        
        similarities = []
        for other_user in range(self.ratings_matrix.shape[0]):
            if other_user != user_id:
                similarity = self._calculate_user_similarity(user_id, other_user)
                if similarity > 0:
                    similarities.append((other_user, similarity))
        
        # 按相似度排序
        similarities.sort(key=lambda x: x[1], reverse=True)
        self.user_similarity_cache[user_id] = similarities
        
        return similarities[:k]
    
    def predict_rating(self, user_id: int, item_id: int) -> float:
        """预测用户对物品的评分"""
        similar_users = self.get_similar_users(user_id)
        
        if not similar_users:
            return 0.0
        
        weighted_sum = 0.0
        similarity_sum = 0.0
        
        for similar_user, similarity in similar_users:
            rating = self.ratings_matrix[similar_user][item_id]
            if rating > 0:
                weighted_sum += similarity * rating
                similarity_sum += abs(similarity)
        
        if similarity_sum == 0:
            return 0.0
        
        return weighted_sum / similarity_sum
    
    def recommend(self, user_id: int, k: int = 10) -> List[Tuple[int, float]]:
        """为用户推荐物品"""
        recommendations = []
        
        for item_id in range(self.ratings_matrix.shape[1]):
            if self.ratings_matrix[user_id][item_id] == 0:  # 用户未评分
                predicted_rating = self.predict_rating(user_id, item_id)
                if predicted_rating > 0:
                    recommendations.append((item_id, predicted_rating))
        
        # 按预测评分排序
        recommendations.sort(key=lambda x: x[1], reverse=True)
        return recommendations[:k]
```

**基于物品的协同过滤（ItemCF）优化版**：
```python
class AdvancedItemCF:
    def __init__(self, ratings_matrix: np.ndarray, min_common_users: int = 5):
        self.ratings_matrix = ratings_matrix
        self.min_common_users = min_common_users
        self.item_similarity_cache = {}
        self.item_popularity = self._calculate_item_popularity()
        
    def _calculate_item_popularity(self) -> Dict[int, float]:
        """计算物品流行度"""
        popularity = {}
        for item_id in range(self.ratings_matrix.shape[1]):
            ratings = self.ratings_matrix[:, item_id]
            popularity[item_id] = np.sum(ratings > 0) / self.ratings_matrix.shape[0]
        return popularity
    
    def _calculate_item_similarity(self, item1: int, item2: int) -> float:
        """计算两个物品的相似度"""
        # 找到对两个物品都评分的用户
        common_users = np.where(
            (self.ratings_matrix[:, item1] > 0) & 
            (self.ratings_matrix[:, item2] > 0)
        )[0]
        
        if len(common_users) < self.min_common_users:
            return 0.0
        
        # 计算余弦相似度
        ratings1 = self.ratings_matrix[common_users, item1]
        ratings2 = self.ratings_matrix[common_users, item2]
        
        dot_product = np.dot(ratings1, ratings2)
        norm1 = np.sqrt(np.sum(ratings1**2))
        norm2 = np.sqrt(np.sum(ratings2**2))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        cosine_sim = dot_product / (norm1 * norm2)
        
        # 应用流行度惩罚
        popularity_penalty = 1.0 / (1.0 + abs(
            self.item_popularity[item1] - self.item_popularity[item2]
        ))
        
        return cosine_sim * popularity_penalty
    
    def get_similar_items(self, item_id: int, k: int = 20) -> List[Tuple[int, float]]:
        """获取最相似的k个物品"""
        if item_id in self.item_similarity_cache:
            return self.item_similarity_cache[item_id][:k]
        
        similarities = []
        for other_item in range(self.ratings_matrix.shape[1]):
            if other_item != item_id:
                similarity = self._calculate_item_similarity(item_id, other_item)
                if similarity > 0:
                    similarities.append((other_item, similarity))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        self.item_similarity_cache[item_id] = similarities
        
        return similarities[:k]
    
    def recommend(self, user_id: int, k: int = 10) -> List[Tuple[int, float]]:
        """为用户推荐物品"""
        user_ratings = self.ratings_matrix[user_id]
        item_scores = defaultdict(float)
        
        # 对用户已评分的物品
        for item_id in np.where(user_ratings > 0)[0]:
            similar_items = self.get_similar_items(item_id)
            
            for similar_item, similarity in similar_items:
                if user_ratings[similar_item] == 0:  # 用户未评分
                    item_scores[similar_item] += similarity * user_ratings[item_id]
        
        # 按分数排序
        recommendations = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)
        return recommendations[:k]
```

### 5.2 矩阵分解算法深度实现

**SVD++算法实现**：
```python
import numpy as np
from typing import Tuple, Optional
import logging

class SVDPlusPlus:
    def __init__(self, R: np.ndarray, K: int = 50, alpha: float = 0.01, 
                 beta: float = 0.01, gamma: float = 0.01, iterations: int = 100):
        self.R = R
        self.num_users, self.num_items = R.shape
        self.K = K
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.iterations = iterations
        
        # 初始化参数
        self.P = np.random.normal(scale=0.1, size=(self.num_users, self.K))
        self.Q = np.random.normal(scale=0.1, size=(self.num_items, self.K))
        self.bu = np.zeros(self.num_users)  # 用户偏置
        self.bi = np.zeros(self.num_items)  # 物品偏置
        self.mu = np.mean(R[R > 0])  # 全局平均分
        
        # SVD++特有参数
        self.Y = np.random.normal(scale=0.1, size=(self.num_items, self.K))
        
        self.logger = logging.getLogger(__name__)
    
    def _get_user_items(self, user_id: int) -> np.ndarray:
        """获取用户评分的物品列表"""
        return np.where(self.R[user_id] > 0)[0]
    
    def _calculate_implicit_feedback(self, user_id: int) -> np.ndarray:
        """计算隐式反馈特征"""
        user_items = self._get_user_items(user_id)
        if len(user_items) == 0:
            return np.zeros(self.K)
        
        # 归一化因子
        sqrt_norm = np.sqrt(len(user_items))
        return np.sum(self.Y[user_items], axis=0) / sqrt_norm
    
    def predict(self, user_id: int, item_id: int) -> float:
        """预测评分"""
        # 基础预测
        base_prediction = self.mu + self.bu[user_id] + self.bi[item_id]
        
        # 显式反馈部分
        explicit_part = np.dot(self.P[user_id], self.Q[item_id])
        
        # 隐式反馈部分
        implicit_feedback = self._calculate_implicit_feedback(user_id)
        implicit_part = np.dot(implicit_feedback, self.Q[item_id])
        
        return base_prediction + explicit_part + implicit_part
    
    def train(self) -> Tuple[np.ndarray, np.ndarray]:
        """训练模型"""
        for iteration in range(self.iterations):
            total_loss = 0
            
            for user_id in range(self.num_users):
                for item_id in range(self.num_items):
                    if self.R[user_id, item_id] > 0:
                        # 计算预测值
                        prediction = self.predict(user_id, item_id)
                        error = self.R[user_id, item_id] - prediction
                        
                        # 计算隐式反馈特征
                        implicit_feedback = self._calculate_implicit_feedback(user_id)
                        
                        # 更新用户特征
                        self.P[user_id] += self.alpha * (
                            error * (self.Q[item_id] + implicit_feedback) - 
                            self.beta * self.P[user_id]
                        )
                        
                        # 更新物品特征
                        self.Q[item_id] += self.alpha * (
                            error * (self.P[user_id] + implicit_feedback) - 
                            self.beta * self.Q[item_id]
                        )
                        
                        # 更新偏置
                        self.bu[user_id] += self.alpha * (error - self.gamma * self.bu[user_id])
                        self.bi[item_id] += self.alpha * (error - self.gamma * self.bi[item_id])
                        
                        # 更新隐式反馈特征
                        user_items = self._get_user_items(user_id)
                        sqrt_norm = np.sqrt(len(user_items))
                        
                        for rated_item in user_items:
                            self.Y[rated_item] += self.alpha * (
                                error * self.Q[item_id] / sqrt_norm - 
                                self.beta * self.Y[rated_item]
                            )
                        
                        total_loss += error ** 2
            
            # 添加正则化项
            total_loss += self.beta * (
                np.sum(self.P ** 2) + np.sum(self.Q ** 2) + 
                np.sum(self.Y ** 2) + np.sum(self.bu ** 2) + np.sum(self.bi ** 2)
            )
            
            if (iteration + 1) % 10 == 0:
                self.logger.info(f"Iteration {iteration + 1}, Loss: {total_loss:.4f}")
        
        return self.P, self.Q
    
    def recommend(self, user_id: int, k: int = 10) -> List[Tuple[int, float]]:
        """为用户推荐物品"""
        predictions = []
        
        for item_id in range(self.num_items):
            if self.R[user_id, item_id] == 0:  # 用户未评分
                prediction = self.predict(user_id, item_id)
                predictions.append((item_id, prediction))
        
        predictions.sort(key=lambda x: x[1], reverse=True)
        return predictions[:k]
```

**NMF（非负矩阵分解）实现**：
```python
class NonNegativeMatrixFactorization:
    def __init__(self, R: np.ndarray, K: int = 50, max_iter: int = 1000, 
                 tolerance: float = 1e-4):
        self.R = R
        self.K = K
        self.max_iter = max_iter
        self.tolerance = tolerance
        self.num_users, self.num_items = R.shape
        
        # 初始化非负矩阵
        self.W = np.random.uniform(0, 1, (self.num_users, self.K))
        self.H = np.random.uniform(0, 1, (self.num_items, self.K))
        
    def _update_W(self):
        """更新用户特征矩阵W"""
        numerator = np.dot(self.R, self.H)
        denominator = np.dot(np.dot(self.W, self.H.T), self.H)
        
        # 避免除零
        denominator = np.where(denominator == 0, 1e-10, denominator)
        
        self.W *= numerator / denominator
    
    def _update_H(self):
        """更新物品特征矩阵H"""
        numerator = np.dot(self.R.T, self.W)
        denominator = np.dot(np.dot(self.H, self.W.T), self.W)
        
        # 避免除零
        denominator = np.where(denominator == 0, 1e-10, denominator)
        
        self.H *= numerator / denominator
    
    def fit(self):
        """训练NMF模型"""
        prev_loss = float('inf')
        
        for iteration in range(self.max_iter):
            # 更新W和H
            self._update_W()
            self._update_H()
            
            # 计算重构误差
            reconstructed = np.dot(self.W, self.H.T)
            mask = self.R > 0
            loss = np.sum((self.R[mask] - reconstructed[mask]) ** 2)
            
            # 检查收敛
            if abs(prev_loss - loss) < self.tolerance:
                print(f"Converged at iteration {iteration + 1}")
                break
            
            prev_loss = loss
            
            if (iteration + 1) % 100 == 0:
                print(f"Iteration {iteration + 1}, Loss: {loss:.4f}")
        
        return self.W, self.H
    
    def predict(self, user_id: int, item_id: int) -> float:
        """预测评分"""
        return np.dot(self.W[user_id], self.H[item_id])
    
    def recommend(self, user_id: int, k: int = 10) -> List[Tuple[int, float]]:
        """推荐物品"""
        predictions = []
        
        for item_id in range(self.num_items):
            if self.R[user_id, item_id] == 0:
                prediction = self.predict(user_id, item_id)
                predictions.append((item_id, prediction))
        
        predictions.sort(key=lambda x: x[1], reverse=True)
        return predictions[:k]
```

## 六、深度学习推荐模型

### 6.1 DeepFM深度实现

```python
import tensorflow as tf
from tensorflow.keras.layers import Dense, Embedding, Input, Concatenate, Multiply, Add
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import numpy as np
from typing import Dict, List, Tuple

class DeepFM:
    def __init__(self, feature_config: Dict, embedding_dim: int = 8, 
                 deep_layers: List[int] = [128, 64, 32], dropout_rate: float = 0.5):
        self.feature_config = feature_config
        self.embedding_dim = embedding_dim
        self.deep_layers = deep_layers
        self.dropout_rate = dropout_rate
        self.model = self._build_model()
    
    def _build_model(self) -> Model:
        """构建DeepFM模型"""
        # 输入层
        inputs = {}
        embeddings = {}
        
        for feature_name, config in self.feature_config.items():
            if config['type'] == 'categorical':
                inputs[feature_name] = Input(shape=(1,), name=f'{feature_name}_input')
                embeddings[feature_name] = Embedding(
                    config['vocab_size'], 
                    self.embedding_dim,
                    name=f'{feature_name}_embedding'
                )(inputs[feature_name])
            else:  # numerical
                inputs[feature_name] = Input(shape=(1,), name=f'{feature_name}_input')
        
        # FM一阶项
        fm_first_order = []
        for feature_name, config in self.feature_config.items():
            if config['type'] == 'categorical':
                fm_first_order.append(embeddings[feature_name])
            else:
                fm_first_order.append(inputs[feature_name])
        
        fm_first_order = Concatenate()(fm_first_order)
        fm_first_order = Dense(1, activation='linear')(fm_first_order)
        
        # FM二阶项
        fm_second_order = []
        for feature_name, config in self.feature_config.items():
            if config['type'] == 'categorical':
                fm_second_order.append(embeddings[feature_name])
        
        if len(fm_second_order) > 1:
            # 计算所有特征对的交互
            interactions = []
            for i in range(len(fm_second_order)):
                for j in range(i + 1, len(fm_second_order)):
                    interaction = Multiply()([fm_second_order[i], fm_second_order[j]])
                    interactions.append(interaction)
            
            fm_second_order = Concatenate()(interactions)
            fm_second_order = Dense(1, activation='linear')(fm_second_order)
        else:
            fm_second_order = Dense(1, activation='linear')(fm_second_order[0])
        
        # Deep部分
        deep_input = []
        for feature_name, config in self.feature_config.items():
            if config['type'] == 'categorical':
                deep_input.append(embeddings[feature_name])
            else:
                deep_input.append(inputs[feature_name])
        
        deep_input = Concatenate()(deep_input)
        
        # Deep网络
        deep_output = deep_input
        for layer_size in self.deep_layers:
            deep_output = Dense(layer_size, activation='relu')(deep_output)
            deep_output = tf.keras.layers.Dropout(self.dropout_rate)(deep_output)
        
        deep_output = Dense(1, activation='linear')(deep_output)
        
        # 组合所有部分
        output = Add()([fm_first_order, fm_second_order, deep_output])
        output = Dense(1, activation='sigmoid')(output)
        
        # 构建模型
        model = Model(inputs=list(inputs.values()), outputs=output)
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy', 'auc']
        )
        
        return model
    
    def train(self, X: Dict[str, np.ndarray], y: np.ndarray, 
              validation_split: float = 0.2, epochs: int = 100, 
              batch_size: int = 1024) -> Dict:
        """训练模型"""
        history = self.model.fit(
            X, y,
            validation_split=validation_split,
            epochs=epochs,
            batch_size=batch_size,
            verbose=1
        )
        
        return history.history
    
    def predict(self, X: Dict[str, np.ndarray]) -> np.ndarray:
        """预测"""
        return self.model.predict(X)
    
    def get_feature_importance(self) -> Dict[str, float]:
        """获取特征重要性"""
        # 这里可以实现特征重要性分析
        # 简化版本，实际应用中需要更复杂的分析
        return {}
```

### 6.2 Wide & Deep深度实现

```python
class WideAndDeep:
    def __init__(self, wide_features: List[str], deep_features: List[str],
                 embedding_config: Dict, deep_layers: List[int] = [128, 64, 32]):
        self.wide_features = wide_features
        self.deep_features = deep_features
        self.embedding_config = embedding_config
        self.deep_layers = deep_layers
        self.model = self._build_model()
    
    def _build_model(self) -> Model:
        """构建Wide & Deep模型"""
        inputs = {}
        
        # 创建输入层
        for feature in self.wide_features + self.deep_features:
            inputs[feature] = Input(shape=(1,), name=f'{feature}_input')
        
        # Wide部分 - 线性模型
        wide_inputs = [inputs[feature] for feature in self.wide_features]
        wide_output = Concatenate()(wide_inputs)
        wide_output = Dense(1, activation='linear')(wide_output)
        
        # Deep部分 - 深度神经网络
        deep_inputs = []
        
        for feature in self.deep_features:
            if feature in self.embedding_config:
                # 分类特征使用embedding
                embedding = Embedding(
                    self.embedding_config[feature]['vocab_size'],
                    self.embedding_config[feature]['embedding_dim']
                )(inputs[feature])
                deep_inputs.append(embedding)
            else:
                # 数值特征直接使用
                deep_inputs.append(inputs[feature])
        
        deep_input = Concatenate()(deep_inputs)
        
        # Deep网络
        deep_output = deep_input
        for layer_size in self.deep_layers:
            deep_output = Dense(layer_size, activation='relu')(deep_output)
            deep_output = tf.keras.layers.Dropout(0.5)(deep_output)
        
        deep_output = Dense(1, activation='linear')(deep_output)
        
        # 组合Wide和Deep
        combined = Concatenate()([wide_output, deep_output])
        output = Dense(1, activation='sigmoid')(combined)
        
        # 构建模型
        model = Model(inputs=list(inputs.values()), outputs=output)
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy', 'auc']
        )
        
        return model
```

### 6.3 Neural Collaborative Filtering (NCF)

```python
class NeuralCollaborativeFiltering:
    def __init__(self, num_users: int, num_items: int, embedding_dim: int = 64,
                 mlp_layers: List[int] = [128, 64, 32], dropout_rate: float = 0.5):
        self.num_users = num_users
        self.num_items = num_items
        self.embedding_dim = embedding_dim
        self.mlp_layers = mlp_layers
        self.dropout_rate = dropout_rate
        self.model = self._build_model()
    
    def _build_model(self) -> Model:
        """构建NCF模型"""
        # 用户和物品输入
        user_input = Input(shape=(1,), name='user_input')
        item_input = Input(shape=(1,), name='item_input')
        
        # Embedding层
        user_embedding = Embedding(self.num_users, self.embedding_dim)(user_input)
        item_embedding = Embedding(self.num_items, self.embedding_dim)(item_input)
        
        # GMF部分 - 广义矩阵分解
        gmf_vector = Multiply()([user_embedding, item_embedding])
        gmf_output = Dense(1, activation='sigmoid')(gmf_vector)
        
        # MLP部分 - 多层感知机
        mlp_vector = Concatenate()([user_embedding, item_embedding])
        
        for layer_size in self.mlp_layers:
            mlp_vector = Dense(layer_size, activation='relu')(mlp_vector)
            mlp_vector = tf.keras.layers.Dropout(self.dropout_rate)(mlp_vector)
        
        mlp_output = Dense(1, activation='sigmoid')(mlp_vector)
        
        # 组合GMF和MLP
        combined = Concatenate()([gmf_vector, mlp_vector])
        output = Dense(1, activation='sigmoid')(combined)
        
        # 构建模型
        model = Model(inputs=[user_input, item_input], outputs=output)
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy', 'auc']
        )
        
        return model
```

## 七、推荐系统架构设计

### 7.1 召回-排序-重排架构

**召回层实现**：
```python
import redis
import numpy as np
from typing import List, Dict, Set
import logging

class RecallService:
    def __init__(self, redis_client: redis.Redis):
        self.redis_client = redis_client
        self.logger = logging.getLogger(__name__)
        
    def collaborative_filtering_recall(self, user_id: int, k: int = 1000) -> List[int]:
        """协同过滤召回"""
        # 从Redis获取用户相似度
        similar_users = self.redis_client.zrevrange(
            f'user_similarity:{user_id}', 0, 99, withscores=True
        )
        
        candidate_items = set()
        for similar_user, similarity in similar_users:
            # 获取相似用户喜欢的物品
            user_items = self.redis_client.smembers(f'user_items:{similar_user}')
            candidate_items.update([int(item) for item in user_items])
        
        # 移除用户已交互的物品
        user_interacted = self.redis_client.smembers(f'user_items:{user_id}')
        candidate_items -= set([int(item) for item in user_interacted])
        
        return list(candidate_items)[:k]
    
    def content_based_recall(self, user_id: int, k: int = 1000) -> List[int]:
        """基于内容召回"""
        # 获取用户偏好标签
        user_tags = self.redis_client.smembers(f'user_tags:{user_id}')
        
        candidate_items = set()
        for tag in user_tags:
            # 获取该标签下的物品
            tag_items = self.redis_client.smembers(f'tag_items:{tag}')
            candidate_items.update([int(item) for item in tag_items])
        
        # 移除用户已交互的物品
        user_interacted = self.redis_client.smembers(f'user_items:{user_id}')
        candidate_items -= set([int(item) for item in user_interacted])
        
        return list(candidate_items)[:k]
    
    def hot_recall(self, k: int = 1000) -> List[int]:
        """热门物品召回"""
        # 从Redis获取热门物品
        hot_items = self.redis_client.zrevrange('hot_items', 0, k-1)
        return [int(item) for item in hot_items]
    
    def multi_recall(self, user_id: int, k: int = 1000) -> List[int]:
        """多路召回"""
        all_candidates = set()
        
        # 协同过滤召回
        cf_items = self.collaborative_filtering_recall(user_id, k//3)
        all_candidates.update(cf_items)
        
        # 内容召回
        content_items = self.content_based_recall(user_id, k//3)
        all_candidates.update(content_items)
        
        # 热门召回
        hot_items = self.hot_recall(k//3)
        all_candidates.update(hot_items)
        
        return list(all_candidates)[:k]
```

**排序层实现**：
```python
import tensorflow as tf
import numpy as np
from typing import List, Dict, Tuple

class RankingService:
    def __init__(self, model_path: str):
        self.model = tf.keras.models.load_model(model_path)
        self.feature_processor = FeatureProcessor()
        
    def extract_features(self, user_id: int, item_ids: List[int]) -> Dict[str, np.ndarray]:
        """提取特征"""
        features = {}
        
        # 用户特征
        user_features = self.feature_processor.get_user_features(user_id)
        features['user_id'] = np.array([user_id] * len(item_ids))
        
        # 物品特征
        item_features = []
        for item_id in item_ids:
            item_feat = self.feature_processor.get_item_features(item_id)
            item_features.append(item_feat)
        
        features['item_id'] = np.array(item_ids)
        features['item_category'] = np.array([feat['category'] for feat in item_features])
        features['item_price'] = np.array([feat['price'] for feat in item_features])
        
        # 交互特征
        interaction_features = []
        for item_id in item_ids:
            interaction_feat = self.feature_processor.get_interaction_features(user_id, item_id)
            interaction_features.append(interaction_feat)
        
        features['user_item_similarity'] = np.array([feat['similarity'] for feat in interaction_features])
        features['user_item_history'] = np.array([feat['history_count'] for feat in interaction_features])
        
        return features
    
    def rank_items(self, user_id: int, item_ids: List[int]) -> List[Tuple[int, float]]:
        """对物品进行排序"""
        if not item_ids:
            return []
        
        # 提取特征
        features = self.extract_features(user_id, item_ids)
        
        # 预测分数
        scores = self.model.predict(features)
        scores = scores.flatten()
        
        # 组合物品ID和分数
        item_scores = list(zip(item_ids, scores))
        
        # 按分数排序
        item_scores.sort(key=lambda x: x[1], reverse=True)
        
        return item_scores
```

**重排序层实现**：
```python
class RerankService:
    def __init__(self):
        self.diversity_weight = 0.3
        self.novelty_weight = 0.2
        self.popularity_weight = 0.1
        
    def calculate_diversity_score(self, items: List[int]) -> float:
        """计算多样性分数"""
        if len(items) <= 1:
            return 1.0
        
        # 获取物品类别
        categories = []
        for item_id in items:
            category = self.get_item_category(item_id)
            categories.append(category)
        
        # 计算类别多样性
        unique_categories = len(set(categories))
        total_items = len(items)
        
        return unique_categories / total_items
    
    def calculate_novelty_score(self, user_id: int, item_id: int) -> float:
        """计算新颖性分数"""
        # 获取用户历史交互
        user_history = self.get_user_history(user_id)
        
        # 计算物品在用户历史中的相似度
        similarities = []
        for hist_item in user_history:
            similarity = self.calculate_item_similarity(item_id, hist_item)
            similarities.append(similarity)
        
        if not similarities:
            return 1.0
        
        # 新颖性 = 1 - 最大相似度
        max_similarity = max(similarities)
        return 1.0 - max_similarity
    
    def calculate_popularity_score(self, item_id: int) -> float:
        """计算流行度分数"""
        # 获取物品的交互次数
        interaction_count = self.get_item_interaction_count(item_id)
        
        # 归一化到0-1
        max_interactions = self.get_max_interaction_count()
        return interaction_count / max_interactions
    
    def rerank_items(self, user_id: int, ranked_items: List[Tuple[int, float]], 
                    k: int = 10) -> List[int]:
        """重排序物品"""
        if len(ranked_items) <= k:
            return [item_id for item_id, _ in ranked_items]
        
        # 计算重排序分数
        rerank_scores = []
        selected_items = []
        
        for item_id, original_score in ranked_items:
            # 多样性分数
            diversity_score = self.calculate_diversity_score(selected_items + [item_id])
            
            # 新颖性分数
            novelty_score = self.calculate_novelty_score(user_id, item_id)
            
            # 流行度分数
            popularity_score = self.calculate_popularity_score(item_id)
            
            # 综合分数
            rerank_score = (
                original_score + 
                self.diversity_weight * diversity_score +
                self.novelty_weight * novelty_score +
                self.popularity_weight * popularity_score
            )
            
            rerank_scores.append((item_id, rerank_score))
        
        # 按重排序分数排序
        rerank_scores.sort(key=lambda x: x[1], reverse=True)
        
        return [item_id for item_id, _ in rerank_scores[:k]]
```

### 7.2 实时推荐系统架构

**实时特征计算**：
```python
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import json
from typing import Dict, List

class RealTimeFeatureProcessor:
    def __init__(self):
        self.redis_client = redis.Redis()
        
    def process_user_behavior(self, behavior_data: Dict) -> None:
        """处理用户行为数据"""
        user_id = behavior_data['user_id']
        item_id = behavior_data['item_id']
        action_type = behavior_data['action_type']
        timestamp = behavior_data['timestamp']
        
        # 更新用户实时特征
        self.update_user_realtime_features(user_id, item_id, action_type, timestamp)
        
        # 更新物品实时特征
        self.update_item_realtime_features(item_id, user_id, action_type, timestamp)
    
    def update_user_realtime_features(self, user_id: int, item_id: int, 
                                    action_type: str, timestamp: int) -> None:
        """更新用户实时特征"""
        # 更新最近点击
        if action_type == 'click':
            self.redis_client.zadd(
                f'user:{user_id}:recent_clicks',
                {str(item_id): timestamp}
            )
            # 只保留最近100个点击
            self.redis_client.zremrangebyrank(f'user:{user_id}:recent_clicks', 0, -101)
        
        # 更新最近购买
        elif action_type == 'purchase':
            self.redis_client.zadd(
                f'user:{user_id}:recent_purchases',
                {str(item_id): timestamp}
            )
            self.redis_client.zremrangebyrank(f'user:{user_id}:recent_purchases', 0, -51)
        
        # 更新用户偏好标签
        item_tags = self.get_item_tags(item_id)
        for tag in item_tags:
            self.redis_client.zincrby(f'user:{user_id}:tag_preference', 1, tag)
    
    def update_item_realtime_features(self, item_id: int, user_id: int,
                                    action_type: str, timestamp: int) -> None:
        """更新物品实时特征"""
        # 更新物品热度
        if action_type == 'click':
            self.redis_client.zincrby('item_hotness', 1, str(item_id))
        
        # 更新物品实时统计
        self.redis_client.hincrby(f'item:{item_id}:stats', f'{action_type}_count', 1)
        
        # 更新物品用户画像
        user_profile = self.get_user_profile(user_id)
        for key, value in user_profile.items():
            self.redis_client.hincrby(f'item:{item_id}:user_profile', key, value)
    
    def get_user_realtime_features(self, user_id: int) -> Dict:
        """获取用户实时特征"""
        features = {}
        
        # 最近点击
        recent_clicks = self.redis_client.zrevrange(
            f'user:{user_id}:recent_clicks', 0, 19, withscores=True
        )
        features['recent_clicks'] = [int(item) for item, _ in recent_clicks]
        
        # 最近购买
        recent_purchases = self.redis_client.zrevrange(
            f'user:{user_id}:recent_purchases', 0, 9, withscores=True
        )
        features['recent_purchases'] = [int(item) for item, _ in recent_purchases]
        
        # 偏好标签
        tag_preferences = self.redis_client.zrevrange(
            f'user:{user_id}:tag_preference', 0, 9, withscores=True
        )
        features['tag_preferences'] = dict(tag_preferences)
        
        return features
```

**实时推荐服务**：
```python
from flask import Flask, request, jsonify
import threading
import time
from typing import Dict, List

class RealTimeRecommendationService:
    def __init__(self):
        self.app = Flask(__name__)
        self.recall_service = RecallService(redis_client)
        self.ranking_service = RankingService(model_path)
        self.rerank_service = RerankService()
        self.feature_processor = RealTimeFeatureProcessor()
        
        # 缓存
        self.recommendation_cache = {}
        self.cache_ttl = 300  # 5分钟
        
        # 启动缓存更新线程
        self.start_cache_update_thread()
    
    def start_cache_update_thread(self):
        """启动缓存更新线程"""
        def update_cache():
            while True:
                try:
                    self.update_recommendation_cache()
                    time.sleep(60)  # 每分钟更新一次
                except Exception as e:
                    print(f"Cache update error: {e}")
                    time.sleep(60)
        
        thread = threading.Thread(target=update_cache, daemon=True)
        thread.start()
    
    def update_recommendation_cache(self):
        """更新推荐缓存"""
        # 获取活跃用户列表
        active_users = self.get_active_users()
        
        for user_id in active_users:
            try:
                # 生成推荐
                recommendations = self.generate_recommendations(user_id)
                
                # 更新缓存
                self.recommendation_cache[user_id] = {
                    'recommendations': recommendations,
                    'timestamp': time.time()
                }
            except Exception as e:
                print(f"Error generating recommendations for user {user_id}: {e}")
    
    def generate_recommendations(self, user_id: int, k: int = 10) -> List[int]:
        """生成推荐"""
        # 1. 召回
        candidate_items = self.recall_service.multi_recall(user_id, k * 10)
        
        # 2. 排序
        ranked_items = self.ranking_service.rank_items(user_id, candidate_items)
        
        # 3. 重排序
        final_items = self.rerank_service.rerank_items(user_id, ranked_items, k)
        
        return final_items
    
    @self.app.route('/recommend', methods=['POST'])
    def recommend(self):
        """推荐接口"""
        try:
            data = request.get_json()
            user_id = data['user_id']
            k = data.get('k', 10)
            
            # 检查缓存
            if user_id in self.recommendation_cache:
                cache_data = self.recommendation_cache[user_id]
                if time.time() - cache_data['timestamp'] < self.cache_ttl:
                    return jsonify({
                        'user_id': user_id,
                        'recommendations': cache_data['recommendations'][:k],
                        'source': 'cache'
                    })
            
            # 实时生成推荐
            recommendations = self.generate_recommendations(user_id, k)
            
            return jsonify({
                'user_id': user_id,
                'recommendations': recommendations,
                'source': 'realtime'
            })
            
        except Exception as e:
            return jsonify({'error': str(e)}), 500
    
    @self.app.route('/feedback', methods=['POST'])
    def feedback(self):
        """用户反馈接口"""
        try:
            data = request.get_json()
            user_id = data['user_id']
            item_id = data['item_id']
            action_type = data['action_type']
            
            # 处理用户行为
            behavior_data = {
                'user_id': user_id,
                'item_id': item_id,
                'action_type': action_type,
                'timestamp': int(time.time())
            }
            
            self.feature_processor.process_user_behavior(behavior_data)
            
            # 清除用户缓存
            if user_id in self.recommendation_cache:
                del self.recommendation_cache[user_id]
            
            return jsonify({'status': 'success'})
            
        except Exception as e:
            return jsonify({'error': str(e)}), 500
```

## 八、冷启动问题解决方案

### 8.1 用户冷启动

**基于人口统计学的推荐**：
```python
class DemographicRecommendation:
    def __init__(self):
        self.demographic_profiles = {}
        self.load_demographic_profiles()
    
    def load_demographic_profiles(self):
        """加载人口统计学档案"""
        # 从数据库加载用户画像
        profiles = self.db.query("""
            SELECT age_group, gender, city, occupation, 
                   AVG(rating) as avg_rating,
                   GROUP_CONCAT(DISTINCT category) as preferred_categories
            FROM users u
            JOIN user_ratings ur ON u.id = ur.user_id
            JOIN items i ON ur.item_id = i.id
            GROUP BY age_group, gender, city, occupation
        """)
        
        for profile in profiles:
            key = f"{profile.age_group}_{profile.gender}_{profile.city}_{profile.occupation}"
            self.demographic_profiles[key] = {
                'avg_rating': profile.avg_rating,
                'preferred_categories': profile.preferred_categories.split(',')
            }
    
    def recommend_for_new_user(self, user_profile: Dict) -> List[int]:
        """为新用户推荐"""
        # 构建用户画像key
        age_group = self.get_age_group(user_profile['age'])
        key = f"{age_group}_{user_profile['gender']}_{user_profile['city']}_{user_profile['occupation']}"
        
        if key in self.demographic_profiles:
            profile = self.demographic_profiles[key]
            
            # 基于偏好类别推荐
            recommendations = []
            for category in profile['preferred_categories']:
                category_items = self.get_popular_items_by_category(category)
                recommendations.extend(category_items)
            
            return recommendations[:10]
        
        # 如果没有匹配的画像，返回热门推荐
        return self.get_hot_items(10)
```

**基于内容的推荐**：
```python
class ContentBasedRecommendation:
    def __init__(self):
        self.item_features = {}
        self.user_profiles = {}
        self.load_item_features()
    
    def load_item_features(self):
        """加载物品特征"""
        items = self.db.query("""
            SELECT id, title, description, category, tags, price
            FROM items
        """)
        
        for item in items:
            # 提取文本特征
            text_features = self.extract_text_features(item.title, item.description)
            
            # 提取类别特征
            category_features = self.extract_category_features(item.category)
            
            # 提取标签特征
            tag_features = self.extract_tag_features(item.tags)
            
            self.item_features[item.id] = {
                'text': text_features,
                'category': category_features,
                'tags': tag_features,
                'price': item.price
            }
    
    def build_user_profile(self, user_id: int) -> Dict:
        """构建用户画像"""
        # 获取用户交互历史
        interactions = self.db.query("""
            SELECT item_id, rating, action_type
            FROM user_interactions
            WHERE user_id = %s
        """, (user_id,))
        
        if not interactions:
            return {}
        
        # 计算用户偏好
        preferences = {
            'categories': defaultdict(float),
            'tags': defaultdict(float),
            'price_range': [0, 0],
            'text_preferences': defaultdict(float)
        }
        
        total_weight = 0
        for interaction in interactions:
            item_id = interaction.item_id
            rating = interaction.rating or 1
            action_type = interaction.action_type
            
            # 计算权重
            weight = self.calculate_interaction_weight(rating, action_type)
            total_weight += weight
            
            # 更新偏好
            item_features = self.item_features[item_id]
            
            # 类别偏好
            preferences['categories'][item_features['category']] += weight
            
            # 标签偏好
            for tag in item_features['tags']:
                preferences['tags'][tag] += weight
            
            # 价格偏好
            price = item_features['price']
            preferences['price_range'][0] += price * weight
            preferences['price_range'][1] += weight
        
        # 归一化
        if total_weight > 0:
            for category in preferences['categories']:
                preferences['categories'][category] /= total_weight
            
            for tag in preferences['tags']:
                preferences['tags'][tag] /= total_weight
            
            preferences['price_range'][0] /= total_weight
        
        return preferences
    
    def recommend_content_based(self, user_id: int, k: int = 10) -> List[int]:
        """基于内容的推荐"""
        user_profile = self.build_user_profile(user_id)
        
        if not user_profile:
            return self.get_hot_items(k)
        
        # 计算物品相似度
        item_scores = {}
        
        for item_id, item_features in self.item_features.items():
            # 检查用户是否已交互
            if self.has_user_interacted(user_id, item_id):
                continue
            
            score = 0
            
            # 类别相似度
            category_score = user_profile['categories'].get(item_features['category'], 0)
            score += category_score * 0.4
            
            # 标签相似度
            tag_score = 0
            for tag in item_features['tags']:
                tag_score += user_profile['tags'].get(tag, 0)
            score += tag_score * 0.3
            
            # 价格相似度
            price = item_features['price']
            avg_price = user_profile['price_range'][0]
            price_similarity = 1 - abs(price - avg_price) / max(price, avg_price, 1)
            score += price_similarity * 0.3
            
            item_scores[item_id] = score
        
        # 按分数排序
        sorted_items = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)
        return [item_id for item_id, _ in sorted_items[:k]]
```

### 8.2 物品冷启动

**基于内容的物品推荐**：
```python
class ItemColdStartRecommendation:
    def __init__(self):
        self.item_content_features = {}
        self.load_content_features()
    
    def load_content_features(self):
        """加载物品内容特征"""
        items = self.db.query("""
            SELECT id, title, description, category, tags, brand, price
            FROM items
        """)
        
        for item in items:
            # 文本特征
            text_features = self.extract_text_features(item.title, item.description)
            
            # 类别特征
            category_features = self.encode_category(item.category)
            
            # 标签特征
            tag_features = self.encode_tags(item.tags)
            
            # 品牌特征
            brand_features = self.encode_brand(item.brand)
            
            self.item_content_features[item.id] = {
                'text': text_features,
                'category': category_features,
                'tags': tag_features,
                'brand': brand_features,
                'price': item.price
            }
    
    def find_similar_items(self, new_item_id: int, k: int = 10) -> List[int]:
        """为新物品找到相似物品"""
        if new_item_id not in self.item_content_features:
            return []
        
        new_item_features = self.item_content_features[new_item_id]
        similarities = []
        
        for item_id, features in self.item_content_features.items():
            if item_id == new_item_id:
                continue
            
            # 计算相似度
            similarity = self.calculate_content_similarity(new_item_features, features)
            similarities.append((item_id, similarity))
        
        # 按相似度排序
        similarities.sort(key=lambda x: x[1], reverse=True)
        return [item_id for item_id, _ in similarities[:k]]
    
    def recommend_for_new_item(self, new_item_id: int, k: int = 100) -> List[int]:
        """为新物品推荐用户"""
        # 找到相似物品
        similar_items = self.find_similar_items(new_item_id, 20)
        
        if not similar_items:
            return self.get_popular_users(k)
        
        # 获取相似物品的用户
        user_scores = defaultdict(float)
        
        for similar_item in similar_items:
            # 获取对该物品有正反馈的用户
            users = self.db.query("""
                SELECT user_id, rating, action_type
                FROM user_interactions
                WHERE item_id = %s AND (rating > 3 OR action_type = 'purchase')
            """, (similar_item,))
            
            for user in users:
                user_id = user.user_id
                rating = user.rating or 1
                action_type = user.action_type
                
                # 计算权重
                weight = self.calculate_interaction_weight(rating, action_type)
                user_scores[user_id] += weight
        
        # 按分数排序
        sorted_users = sorted(user_scores.items(), key=lambda x: x[1], reverse=True)
        return [user_id for user_id, _ in sorted_users[:k]]
```

## 九、推荐系统评估指标

### 9.1 离线评估指标

**准确率指标**：
```python
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error
from typing import List, Dict, Tuple

class OfflineEvaluation:
    def __init__(self):
        self.metrics = {}
    
    def calculate_rmse(self, true_ratings: List[float], 
                      predicted_ratings: List[float]) -> float:
        """计算RMSE"""
        return np.sqrt(mean_squared_error(true_ratings, predicted_ratings))
    
    def calculate_mae(self, true_ratings: List[float], 
                     predicted_ratings: List[float]) -> float:
        """计算MAE"""
        return mean_absolute_error(true_ratings, predicted_ratings)
    
    def calculate_precision_at_k(self, true_items: List[int], 
                                predicted_items: List[int], k: int) -> float:
        """计算Precision@K"""
        if k > len(predicted_items):
            k = len(predicted_items)
        
        top_k_predicted = predicted_items[:k]
        true_set = set(true_items)
        
        hits = len(set(top_k_predicted) & true_set)
        return hits / k
    
    def calculate_recall_at_k(self, true_items: List[int], 
                             predicted_items: List[int], k: int) -> float:
        """计算Recall@K"""
        if k > len(predicted_items):
            k = len(predicted_items)
        
        top_k_predicted = predicted_items[:k]
        true_set = set(true_items)
        
        hits = len(set(top_k_predicted) & true_set)
        return hits / len(true_set) if len(true_set) > 0 else 0
    
    def calculate_f1_at_k(self, true_items: List[int], 
                         predicted_items: List[int], k: int) -> float:
        """计算F1@K"""
        precision = self.calculate_precision_at_k(true_items, predicted_items, k)
        recall = self.calculate_recall_at_k(true_items, predicted_items, k)
        
        if precision + recall == 0:
            return 0
        
        return 2 * precision * recall / (precision + recall)
    
    def calculate_ndcg_at_k(self, true_items: List[int], 
                           predicted_items: List[int], k: int) -> float:
        """计算NDCG@K"""
        if k > len(predicted_items):
            k = len(predicted_items)
        
        # 计算DCG
        dcg = 0
        for i in range(k):
            item = predicted_items[i]
            if item in true_items:
                dcg += 1 / np.log2(i + 2)  # i+2 because log2(1) = 0
        
        # 计算IDCG
        idcg = 0
        for i in range(min(k, len(true_items))):
            idcg += 1 / np.log2(i + 2)
        
        return dcg / idcg if idcg > 0 else 0
    
    def calculate_map_at_k(self, true_items: List[int], 
                          predicted_items: List[int], k: int) -> float:
        """计算MAP@K"""
        if k > len(predicted_items):
            k = len(predicted_items)
        
        true_set = set(true_items)
        precision_sum = 0
        hits = 0
        
        for i in range(k):
            if predicted_items[i] in true_set:
                hits += 1
                precision_sum += hits / (i + 1)
        
        return precision_sum / len(true_set) if len(true_set) > 0 else 0
    
    def evaluate_model(self, model, test_data: List[Dict], k: int = 10) -> Dict[str, float]:
        """评估模型"""
        metrics = {
            'precision': [],
            'recall': [],
            'f1': [],
            'ndcg': [],
            'map': []
        }
        
        for test_case in test_data:
            user_id = test_case['user_id']
            true_items = test_case['true_items']
            
            # 生成推荐
            predicted_items = model.recommend(user_id, k * 2)
            
            # 计算指标
            metrics['precision'].append(
                self.calculate_precision_at_k(true_items, predicted_items, k)
            )
            metrics['recall'].append(
                self.calculate_recall_at_k(true_items, predicted_items, k)
            )
            metrics['f1'].append(
                self.calculate_f1_at_k(true_items, predicted_items, k)
            )
            metrics['ndcg'].append(
                self.calculate_ndcg_at_k(true_items, predicted_items, k)
            )
            metrics['map'].append(
                self.calculate_map_at_k(true_items, predicted_items, k)
            )
        
        # 计算平均指标
        avg_metrics = {}
        for metric_name, values in metrics.items():
            avg_metrics[f'avg_{metric_name}'] = np.mean(values)
            avg_metrics[f'std_{metric_name}'] = np.std(values)
        
        return avg_metrics
```

### 9.2 在线评估指标

**A/B测试框架**：
```python
import random
import time
from typing import Dict, List
import logging

class ABTestFramework:
    def __init__(self):
        self.experiments = {}
        self.logger = logging.getLogger(__name__)
    
    def create_experiment(self, experiment_name: str, 
                         variants: List[Dict], traffic_split: List[float]) -> str:
        """创建A/B测试实验"""
        if len(variants) != len(traffic_split):
            raise ValueError("Variants and traffic split must have same length")
        
        if abs(sum(traffic_split) - 1.0) > 1e-6:
            raise ValueError("Traffic split must sum to 1.0")
        
        experiment = {
            'name': experiment_name,
            'variants': variants,
            'traffic_split': traffic_split,
            'start_time': time.time(),
            'status': 'running',
            'metrics': {}
        }
        
        self.experiments[experiment_name] = experiment
        self.logger.info(f"Created experiment: {experiment_name}")
        
        return experiment_name
    
    def assign_user_to_variant(self, user_id: int, experiment_name: str) -> str:
        """为用户分配实验变体"""
        if experiment_name not in self.experiments:
            return 'control'
        
        experiment = self.experiments[experiment_name]
        
        if experiment['status'] != 'running':
            return 'control'
        
        # 使用用户ID的哈希值确保一致性
        hash_value = hash(str(user_id) + experiment_name) % 100
        
        cumulative_split = 0
        for i, split in enumerate(experiment['traffic_split']):
            cumulative_split += split * 100
            if hash_value < cumulative_split:
                return experiment['variants'][i]['name']
        
        return experiment['variants'][-1]['name']
    
    def track_event(self, user_id: int, experiment_name: str, 
                   event_type: str, event_data: Dict = None) -> None:
        """跟踪用户事件"""
        variant = self.assign_user_to_variant(user_id, experiment_name)
        
        if experiment_name not in self.experiments:
            return
        
        experiment = self.experiments[experiment_name]
        
        # 初始化指标
        if variant not in experiment['metrics']:
            experiment['metrics'][variant] = {
                'users': set(),
                'events': defaultdict(int),
                'conversions': defaultdict(int)
            }
        
        # 记录用户
        experiment['metrics'][variant]['users'].add(user_id)
        
        # 记录事件
        experiment['metrics'][variant]['events'][event_type] += 1
        
        # 记录转化
        if event_type in ['click', 'purchase', 'subscribe']:
            experiment['metrics'][variant]['conversions'][event_type] += 1
    
    def calculate_metrics(self, experiment_name: str) -> Dict[str, Dict]:
        """计算实验指标"""
        if experiment_name not in self.experiments:
            return {}
        
        experiment = self.experiments[experiment_name]
        results = {}
        
        for variant, metrics in experiment['metrics'].items():
            user_count = len(metrics['users'])
            
            if user_count == 0:
                results[variant] = {'error': 'No users in variant'}
                continue
            
            variant_results = {
                'user_count': user_count,
                'events_per_user': {},
                'conversion_rates': {}
            }
            
            # 计算每用户事件数
            for event_type, count in metrics['events'].items():
                variant_results['events_per_user'][event_type] = count / user_count
            
            # 计算转化率
            for event_type, count in metrics['conversions'].items():
                variant_results['conversion_rates'][event_type] = count / user_count
            
            results[variant] = variant_results
        
        return results
    
    def statistical_significance_test(self, experiment_name: str, 
                                    metric: str, alpha: float = 0.05) -> Dict:
        """统计显著性检验"""
        results = self.calculate_metrics(experiment_name)
        
        if len(results) < 2:
            return {'error': 'Need at least 2 variants for significance test'}
        
        variants = list(results.keys())
        control_variant = variants[0]
        treatment_variant = variants[1]
        
        # 获取指标数据
        control_metric = results[control_variant].get(metric, 0)
        treatment_metric = results[treatment_variant].get(metric, 0)
        
        # 计算提升
        if control_metric > 0:
            lift = (treatment_metric - control_metric) / control_metric
        else:
            lift = 0
        
        # 简化的显著性检验（实际应用中应使用更严格的统计方法）
        is_significant = abs(lift) > 0.05  # 5%提升阈值
        
        return {
            'control_metric': control_metric,
            'treatment_metric': treatment_metric,
            'lift': lift,
            'is_significant': is_significant,
            'confidence_level': 1 - alpha
        }
```

## 十、高频面试题深度解析

### Q1：推荐系统的召回和排序有什么区别？

**召回（Recall）**：
- **目标**：从海量物品中快速筛选出候选集
- **特点**：速度快、覆盖面广、精度要求不高
- **方法**：协同过滤、内容推荐、热门推荐、地理位置推荐
- **数量级**：从百万级物品中筛选出几千个候选物品

**排序（Ranking）**：
- **目标**：对候选集进行精确排序
- **特点**：精度要求高、可以使用复杂特征和模型
- **方法**：机器学习模型（LR、GBDT、DNN）
- **数量级**：从几千个候选物品中排序出最终推荐

**代码示例**：
```python
class RecommendationPipeline:
    def __init__(self):
        self.recall_service = RecallService()
        self.ranking_service = RankingService()
        self.rerank_service = RerankService()
    
    def recommend(self, user_id: int, k: int = 10) -> List[int]:
        # 1. 召回：从百万物品中筛选出1000个候选
        candidates = self.recall_service.multi_recall(user_id, k * 100)
        
        # 2. 排序：对1000个候选进行精确排序
        ranked_items = self.ranking_service.rank_items(user_id, candidates)
        
        # 3. 重排序：考虑多样性、新颖性等因素
        final_items = self.rerank_service.rerank_items(user_id, ranked_items, k)
        
        return final_items
```

### Q2：如何处理推荐系统的冷启动问题？

**用户冷启动**：
1. **基于人口统计学**：年龄、性别、地域、职业
2. **基于内容**：用户填写的兴趣标签
3. **热门推荐**：推荐当前热门物品
4. **引导用户**：通过问卷收集用户偏好

**物品冷启动**：
1. **基于内容**：利用物品的文本、图像、类别信息
2. **专家推荐**：人工标注或专家评分
3. **相似物品**：找到内容相似的物品，推荐给喜欢这些物品的用户
4. **探索策略**：ε-greedy算法平衡探索和利用

**代码示例**：
```python
class ColdStartHandler:
    def handle_user_cold_start(self, user_profile: Dict) -> List[int]:
        """处理用户冷启动"""
        # 1. 基于人口统计学推荐
        demographic_recs = self.demographic_recommendation(user_profile)
        
        # 2. 基于内容推荐
        content_recs = self.content_based_recommendation(user_profile)
        
        # 3. 热门推荐
        hot_recs = self.get_hot_items(10)
        
        # 组合推荐
        all_recs = demographic_recs + content_recs + hot_recs
        return list(set(all_recs))[:20]
    
    def handle_item_cold_start(self, item_id: int) -> List[int]:
        """处理物品冷启动"""
        # 1. 找到相似物品
        similar_items = self.find_similar_items(item_id)
        
        # 2. 获取相似物品的用户
        target_users = []
        for similar_item in similar_items:
            users = self.get_item_users(similar_item)
            target_users.extend(users)
        
        # 3. 去重并排序
        return list(set(target_users))[:100]
```

### Q3：推荐系统如何评估效果？

**离线评估**：
- **准确率指标**：RMSE、MAE
- **排序指标**：Precision@K、Recall@K、NDCG@K、MAP@K
- **覆盖率**：推荐物品的多样性
- **新颖性**：推荐用户未接触过的物品

**在线评估**：
- **点击率（CTR）**：用户点击推荐物品的比例
- **转化率（CVR）**：用户购买推荐物品的比例
- **停留时间**：用户在推荐页面停留的时间
- **A/B测试**：对比不同推荐策略的效果

**代码示例**：
```python
class RecommendationEvaluator:
    def offline_evaluation(self, model, test_data: List[Dict]) -> Dict[str, float]:
        """离线评估"""
        metrics = {
            'precision_at_10': [],
            'recall_at_10': [],
            'ndcg_at_10': [],
            'coverage': [],
            'novelty': []
        }
        
        for test_case in test_data:
            user_id = test_case['user_id']
            true_items = test_case['true_items']
            
            # 生成推荐
            recommendations = model.recommend(user_id, 10)
            
            # 计算指标
            metrics['precision_at_10'].append(
                self.calculate_precision_at_k(true_items, recommendations, 10)
            )
            metrics['recall_at_10'].append(
                self.calculate_recall_at_k(true_items, recommendations, 10)
            )
            metrics['ndcg_at_10'].append(
                self.calculate_ndcg_at_k(true_items, recommendations, 10)
            )
        
        # 计算平均指标
        avg_metrics = {}
        for metric_name, values in metrics.items():
            avg_metrics[f'avg_{metric_name}'] = np.mean(values)
        
        return avg_metrics
    
    def online_evaluation(self, experiment_data: Dict) -> Dict[str, float]:
        """在线评估"""
        metrics = {
            'ctr': experiment_data['clicks'] / experiment_data['impressions'],
            'cvr': experiment_data['conversions'] / experiment_data['clicks'],
            'avg_session_duration': experiment_data['total_duration'] / experiment_data['sessions'],
            'revenue_per_user': experiment_data['total_revenue'] / experiment_data['users']
        }
        
        return metrics
```

### Q4：推荐系统如何处理数据稀疏性问题？

**数据稀疏性问题**：
- 用户-物品交互矩阵非常稀疏
- 大部分用户只与少数物品交互
- 新用户和新物品缺乏历史数据

**解决方案**：

1. **矩阵分解**：
```python
class SparseDataHandler:
    def matrix_factorization(self, sparse_matrix: np.ndarray, k: int = 50) -> Tuple[np.ndarray, np.ndarray]:
        """矩阵分解处理稀疏数据"""
        # 使用SVD或NMF分解稀疏矩阵
        from sklearn.decomposition import TruncatedSVD
        
        svd = TruncatedSVD(n_components=k, random_state=42)
        user_features = svd.fit_transform(sparse_matrix)
        item_features = svd.components_.T
        
        return user_features, item_features
```

2. **正则化**：
```python
def regularized_matrix_factorization(self, R: np.ndarray, k: int = 50, 
                                   lambda_reg: float = 0.01) -> Tuple[np.ndarray, np.ndarray]:
    """正则化矩阵分解"""
    # 添加L2正则化项防止过拟合
    loss = mse_loss + lambda_reg * (l2_norm(P) + l2_norm(Q))
```

3. **数据增强**：
```python
def data_augmentation(self, user_interactions: List[Dict]) -> List[Dict]:
    """数据增强"""
    augmented_data = []
    
    for interaction in user_interactions:
        # 原始数据
        augmented_data.append(interaction)
        
        # 添加噪声
        noisy_interaction = interaction.copy()
        noisy_interaction['rating'] += np.random.normal(0, 0.1)
        augmented_data.append(noisy_interaction)
        
        # 时间窗口扩展
        time_window_interaction = interaction.copy()
        time_window_interaction['timestamp'] += np.random.randint(-3600, 3600)
        augmented_data.append(time_window_interaction)
    
    return augmented_data
```

### Q5：推荐系统如何保证实时性？

**实时性要求**：
- 用户行为发生后，推荐结果需要快速更新
- 新物品上线后，需要快速进入推荐候选集
- 热门趋势变化时，推荐结果需要及时调整

**技术方案**：

1. **流式处理**：
```python
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions

class RealTimeRecommendationPipeline:
    def __init__(self):
        self.pipeline_options = PipelineOptions()
    
    def process_user_behavior_stream(self):
        """处理用户行为流"""
        with beam.Pipeline(options=self.pipeline_options) as pipeline:
            # 读取Kafka流
            events = (pipeline
                     | 'ReadFromKafka' >> beam.io.ReadFromKafka(
                         consumer_config={'bootstrap.servers': 'localhost:9092'},
                         topics=['user_behavior']
                     ))
            
            # 实时特征计算
            features = (events
                       | 'ExtractFeatures' >> beam.Map(self.extract_realtime_features)
                       | 'UpdateUserProfile' >> beam.Map(self.update_user_profile))
            
            # 更新推荐缓存
            (features
             | 'UpdateRecommendations' >> beam.Map(self.update_recommendations)
             | 'WriteToRedis' >> beam.Map(self.write_to_redis))
```

2. **缓存策略**：
```python
class RecommendationCache:
    def __init__(self):
        self.redis_client = redis.Redis()
        self.cache_ttl = 300  # 5分钟
    
    def get_cached_recommendations(self, user_id: int) -> Optional[List[int]]:
        """获取缓存的推荐"""
        cache_key = f'recommendations:{user_id}'
        cached_data = self.redis_client.get(cache_key)
        
        if cached_data:
            return json.loads(cached_data)
        return None
    
    def cache_recommendations(self, user_id: int, recommendations: List[int]) -> None:
        """缓存推荐结果"""
        cache_key = f'recommendations:{user_id}'
        self.redis_client.setex(
            cache_key, 
            self.cache_ttl, 
            json.dumps(recommendations)
        )
    
    def invalidate_user_cache(self, user_id: int) -> None:
        """使用户缓存失效"""
        cache_key = f'recommendations:{user_id}'
        self.redis_client.delete(cache_key)
```

3. **增量更新**：
```python
class IncrementalUpdate:
    def __init__(self):
        self.user_profiles = {}
        self.item_profiles = {}
    
    def incremental_update_user_profile(self, user_id: int, 
                                      new_interaction: Dict) -> None:
        """增量更新用户画像"""
        if user_id not in self.user_profiles:
            self.user_profiles[user_id] = self.initialize_user_profile()
        
        profile = self.user_profiles[user_id]
        
        # 更新偏好标签
        item_tags = self.get_item_tags(new_interaction['item_id'])
        for tag in item_tags:
            profile['tag_preferences'][tag] = (
                profile['tag_preferences'].get(tag, 0) * 0.9 +  # 衰减
                new_interaction['weight'] * 0.1  # 新权重
            )
        
        # 更新最近行为
        profile['recent_actions'].append(new_interaction)
        if len(profile['recent_actions']) > 100:
            profile['recent_actions'] = profile['recent_actions'][-100:]
    
    def incremental_update_item_profile(self, item_id: int, 
                                      new_interaction: Dict) -> None:
        """增量更新物品画像"""
        if item_id not in self.item_profiles:
            self.item_profiles[item_id] = self.initialize_item_profile()
        
        profile = self.item_profiles[item_id]
        
        # 更新热度
        profile['hotness'] = profile['hotness'] * 0.95 + new_interaction['weight'] * 0.05
        
        # 更新用户画像
        user_profile = self.get_user_profile(new_interaction['user_id'])
        for key, value in user_profile.items():
            profile['user_profile'][key] = (
                profile['user_profile'].get(key, 0) * 0.9 + value * 0.1
            )
```

---

**关键字**：推荐系统、协同过滤、矩阵分解、深度学习、召回排序、冷启动、评估指标、A/B测试、实时推荐、特征工程、用户画像、物品画像

